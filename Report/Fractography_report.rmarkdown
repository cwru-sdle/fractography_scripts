---
title: Predictive Model of Fatigue for Laser Powder Bed Fusion Samples
date: 2024-10-30
author:
    name: "Anthony Lino"
    email: aml334@case.edu
    corresponding: true
format:
  html:
    css: style.css
    embed-resources: true
    code-overflow: wrap
    toc: true
    toc-depth: 2
    toc-title: Table of Contents
    toc-location: right
bibliography: references.bib
csl: ieee.csl
css: style.css
---



# Excuetive Summary

- Research Objective:
    - Develop a predictive model for fatigue performance in Laser Powder Bed Fusion (LPBF) samples
    - Address literature gap in process parameter optimization for fatigue properties
- Approach:
    - Segmentation of the initiating defect with Meta's Segment Anything Model 
    - Hypothesis testing to correlate defect features with mechanical properties
- Key Findings:
    - Developed predictive model with polynomial regression
    - Achieved R² of 0.465
    - Demonstrated marginal improvement in prediction of cycles to failure through extracted features

# Abstract

Fatigue is a well-known failure mechanism for commercial equipment, where a material fails over a period of time due to a progressive deterioration, through high and low stress cycles. Laser Powder Bed Fusion (LPBF) is an advanced manufacturing process that has great potential for structural optimization, especially in the aerospace sector. The LPBF process also leads to increased ramp surfaces and internal defects, which adversely impact the fatigue performance of parts produced.

This research fills a major gap within the literature by investigating how set of manufacturing parameters define the defect characteristics and the overall fatigue performance. The key tasks comprised of defect segmentation, progression of initiating defects, and regions of fatigue crack growth evolution characterization.

In this study, the data extraction defect features were correlated to mechanical properties using image processing, machine learning segmentation and hypothesis testing. A deep learning approach that employs Attention U-Net architecture for the segmentation of fatigue regions was shown to be more effective than traditional U-Net models because of the importance of global context. The features improve prediction of fatigue performance, with the $R^2$ incraesing by 0.02.

# Introduction

## Fatigue

Fatigue is a common failure mechanism for commercial equipment. Fatigue is a phenomenon where cracks will grow as they solid experiences cycles of high and low stress, gradually weakening the solid and eventually causing failure well below the expected strength of the material. This is shown in figure 1, where the stress experienced near the crack tip is much greater than the stress experienced throughout the rest of the material, and the ratio between these two values is called the stress concentration factor (SCF).



```{r Stress Concentration Factor,out.width="50%"}
#| echo: false
#| fig-cap: "In the left figure the horizontal axis is the distance from the crack in the plane of the crack tip and the vertical axis is the stress as that position. We can see that as we get cloaser to the crack, the stress experienced at that point will go up to a maximum stress. On the right figure, this is visualized with force lines, which are meant to represent the force passing in a straight line from the bottom to the top of the material. Near the crack tip, several of the force lines are cut off, and are forced to pool near the crack tip, which causes the higher experienced stress."
library(knitr)
knitr::include_graphics("Figures/Stress_Concentration_factor.png")
```



As the material is stressed, the crack will grow, but eventually reach an equilibrium when the material stops stretching. This growth is also not linear, since the larger the crack the greater the concentration and the faster the growth. This feedback loop causes exponential crack growth, which shows up as linear on a log-log plot, such as figure 2. While this is easiest to explain with cracks, any irregularity in a solid, including sharp edges, surface roughness and internal defects can concentrate stress, which can cause a crack to form, and the crack can grow from there. As a result of this exponential relationship, the majority of the cycles are spent forming an initial crack from a defect, called crack nucleation, and when the crack is small. As a result, the shape, which determines the stress concentration, and the defect size, which determines the starting size of the crack, are key determiners of fatigue performance.



```{r Paris Regime}
#| echo: false
#| fig-cap: "a is the crack length, N is the cycle and K is the stress intensity factor, which estimates the stress experienced near the tip of the crack based on it's shape length and the applied stress. This means that d(a)/dN is the growth in crack length per cycle and ΔK is the change in stress near the crack tip at the high and low point of the stress. This linear relationship near the center is called the Paris Regime and is a property of the material."
knitr::include_graphics("Figures/Crack-growth-curve-for-three-crack-propagation-regions.png")
```



## Laser Powder Bed Fusion

Laser Powder Bed Fusion is (LPBF) an emerging manufacturing method which allows for straightforward small batch manufacturing of complex designs. The ease of manufacturing small batches and the high design freedom allows for high performance designs. This makes it particularly well suited for adoption in the Aerospace industry. However, fatigue performance is a key metric in the Aerospace industry since cyclic loading is ubiquitous in Aerospace applications. This is problematic because LPBF inherently causes a rough surface and defects, which cause poor fatigue properties. A rough surface can be polished, but defects can be embedded below the surface, making them significantly harder to fix. The number of defects can be reduced by optimizing the settings used for printing, but that can be a very time consuming process. As a result, most studies focusing on process parameter optimization rely on these easier tests, leaving a gap in the literature for studies on fatigue optimization. A study in Professor Lewandowski’s lab addressed this by tested the impact of different processing parameters on the fatigue properties. After fracture, the fractured surfaces were imaged under a microscope, revealing the impact of different defects.

## Quantitative Fractography

Fractography is the study of fracture surfaces. This is a largely qualitative field, but there is a emerging field of quantitative fractography which relies on segmenting these images, and then extracting features from these masks. The Lewandowski lab explored the use of quantitative fractography for these images by performing 4 tasks:

1. Segmenting every defect from the fracture surface
2. Segmenting only the initiating defect from the fracture surface
3. Segmenting the region of fatigue crack growth
4. Segmenting the region of overload 

While significant efforts were made, this is a time consuming process, it can take upwards of 4 hours for a single sample, making use of the entire dataset unfeasible. The existing manually annotated data can be used to train machine learning models to perform this segmentation task on the remainder of the data, allowing conclusions to be drawn on the remainder of the data. This is the primary task done in the remainder of this data.

# Data Science Methods

Machine learning is used to segment defects and interesting characteristics from the images.
Hypothesis testing is used to show the correlation between the extracted defects and the resulting mechanical properties.

## Python Packages

- os - used for reading paths 
- sys - used for adding folders to path 
- cv2 - the most powerful library for image processing currently available in terms of both performance and capability 
- numpy - allows images to be eﬀiciently worked with as arrays. The foundation for cv2 
- pandas - the most popular python library for working with dataframes 
- math - used for mathematical functions 
- random - used to create random variables 
- matplotlib.pyplot - used for plotting
- re - python implementation of regex, a syntax for string processing
- datetime - used as a stopwatch to track performance
- ast - used to read in lists
- scipy - used for statistical tests
- seaborn - used to visualize standard statistical tests
- math - provides logarithmic and trigonometric functions
- joblib - package for parallelization of python scripts

organize_data is a script used to create the dataframe used in the rest of the report. It is imported here because some functions used to organize the data are also helpful in analysis.



```{python package import,echo=FALSE}
import os
import sys
import cv2
import numpy as np
import pandas as pd
import math
import random
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import matplotlib.gridspec as gridspec
import re
import ast
import scipy
import seaborn
import math
import joblib
import torch
sys.path.append("/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/fractography_scripts")
import organize_data
import initiating_defect_features
import initiating_defect_mask_validation
from segment_anything import sam_model_registry, SamPredictor
```



In addition to these package, the recent release of Meta's foundation models segment anything and segment anything 2 will be explored for use in characterizing the unsegmented initiating defects from high resolution images. [@kirillov_segment_2023] [@ravi2024sam2]

# Exploratory Data Analysis

## Explanation of your data set

The dataset mostly consists of unorganized images in 2 folders, and the numerical data is stored in a handful of spreadsheets. The dataframe was created in two parts: 1) The image dataframe, which contains the path to the images and 2) the numerical dataframe where are spreadsheets were merged. The id used to merge these dataframes is the sample_id, which was extracted from the image path using a regex and was cleaned from the existing Sample# column in the numerical spreadsheets. For the image dataframes, different categories were made, and each has a corresponding function which takes a file path as input an returns true for whether the path leads to an image of that category. Data validation is done here to ensure that the categories are well defined. 

## Data Cleaning

In addition to these package, the recent release of Meta's foundation models segment anything and segment anything 2 will be explored for use in characterizing the unsegmented initiating defects from high resolution images. [@kirillov_segment_2023] [@ravi2024sam2]


Data cleaning was an iterative process, with the organize data script being run to create a dataframe, then the results analyzed and verified with a jupyter notebook. The organize data script is in it's own section, and the smaller chucks used to analyze the results are below it. Several sections are not fully cleaned, with the majority of the effort going to the fatigue and overload region, since they are the easiest to verify and should be the simplest task for the model since they are rather large.
Some forms of data validation include:

1. Use of a size threshold to validate the separation between stitched low resolution images and individual low resolution images.
2. A lower red pixel threshold to remove pixels in the greyscale images which are identified as colored because of noise and a higher red pixel threshold to identify images where every defect is segmented versus only the initiating defect segmented.

The full data cleaning and formation of the combined_df can be seen in organize_data.py

## Dataset Counts



```{python import data,error=FALSE,echo=FALSE}
combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
organize_data.print_column_counts(combined_df)
print(combined_df['image_class'].value_counts().rename("All image types"))
print('Unique samples: ' +str(combined_df['sample_id'].nunique()))
with_polygon_df = combined_df[~combined_df['points'].isna()]
with_polygon_df['image_class'].value_counts().rename("SAM Segmented Image Types")
```



## Difference in Cycles to Failure

Below is a histogram and and scatter plot of the difference of two samples which had identical manufacturing and testing conditions. While we can already see some trends emerging, with samples with both very high and low energy having worse fatigue performance, but the variation is still largely unaccounted for.



```{python parameters vs cycles,error=FALSE,results='hide',echo=FALSE}
xy_df = combined_df[~combined_df['energy_density_J_mm3'].isna() & ~combined_df['cycles'].isna()]
plt.rcParams.update({'font.size': 32})  # Set font size for all elements
hist_x = []
bin_name = []
for group_string, row in combined_df.groupby(['scan_power_W','scan_velocity_mm_s','test_stress_Mpa']):
    row = row[['scan_power_W','scan_velocity_mm_s','test_stress_Mpa','cycles']].drop_duplicates().reset_index(drop=True)
    variable_cycles = list(row['cycles'].value_counts().index)
    if(len(variable_cycles)>1):
        hist_x.append(variable_cycles)
        bin_name.append(len(variable_cycles))
box_fig, box_ax = plt.subplots( figsize=(16, 8))
box_ax.tick_params(axis='x',bottom=False,labelbottom=False,)
# box_ax.yaxis.set_major_locator(ticker.MultipleLocator(80))
box_ax.set_ylabel('Cycles to Failure')
box_ax.set_xlabel('Unique Processing and Testing Conditions')
box_ax.boxplot(hist_x)
box_fig.show()
```

```{python cucles vs energy density,error=FALSE,results='hide',echo=FALSE}
xy_df['cycles_stress'] = xy_df['cycles'].apply(lambda x: math.log(x))*xy_df['test_stress_Mpa']
SN_fig, SN_ax = plt.subplots(figsize=(16, 8))
SN_ax.scatter(xy_df['energy_density_J_mm3'],xy_df['cycles_stress'])
slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(xy_df['energy_density_J_mm3'],xy_df['cycles_stress'])
x = np.linspace(20, 120, 100)
SN_ax.plot(x, slope * x + intercept, label=f'Linear fit: y = {slope:.2f}x + {intercept:.2f}', color='red')
SN_ax.set_ylabel('log(Cycles)*stress [Mpa]')
SN_ax.set_xlabel('Energy Density [J/mm^3]')
SN_ax.text(SN_ax.get_xlim()[1],SN_ax.get_ylim()[1],f"R^2={r_value:.2f}", fontsize = 24,ha='right', va='top')
SN_fig.tight_layout(h_pad=3)
SN_fig.show()
```



The above figures show that the spreadsheets do not contain all of the necessary information, and from theory, we know that defects may be a leading cause. This is commonly visualized using log(stress) vs log(cycle) (SN) plots. We can also color the points according to their energy density.



```{python SN curve,error=FALSE,results='hide',echo=FALSE}
cmap = plt.get_cmap('inferno')

fig, (ax_scatter,ax_hist_x) = plt.subplots(2,1,figsize=(16,10),height_ratios=[3,1])

groups = combined_df.sort_values(by='cycles').groupby(['energy_density_J_mm3'])
for i, (group, group_data) in enumerate(groups):
        Cycles = group_data['cycles']
        Stress = list(map(math.log10,group_data['test_stress_Mpa']))
        ax_scatter.scatter(
            Cycles,
            Stress,
            color=cmap(group_data['energy_density_J_mm3']/100))
ax_scatter.set_ylabel('Stress [Mpa]')
# ax_scatter.set_xlabel('log(Cycles)')
ax_scatter.set_title('Log-Log Scatter Plot of SN Data')
# colorbar = fig.colorbar(ax.collections[0], ax=ax)
ax_hist_x.hist(list(map(math.log10, combined_df['cycles'])), bins=30, orientation='vertical')
ax_hist_x.set_ylabel('Frequency')
ax_hist_x.set_xlabel("log(Cycles)")
plt.subplots_adjust(hspace=0)
plt.show()
```



Doing this, we can see that that the purple points, which is the samples with the most energy that have keyhold defects, were generally tested at a lower stress and failed with few cycles, while those with the least energy that have lack of fusion defects failed at higher stress with few cycles. The strongest samples, those tested at the highest stress and lasted the longest tend to be shades of orange. However, we can again see a lot of noise with some purple and yellow points being almost as strong as the other samples.

# Statistical Learning

## Fatigue Ratio

### Segmenting Fatigue Region

The fatigue region was selected as the first segmentation task because it is correlated with the fracture toughness, which is determined by the micro structure, which is otherwise unaccounted for. Since a tough micro structure is known to inhibit fatigue crack growth, this is a good candidate for a predictor of Cycles to failure difference. Additionally, the fatigue region is the largest part of the image and is the cleanest of the columns, so is the easiest to work with.
The fatigue_training.py is the full training script and is attatched as an appendix. Data augmentation was used to make the most of the small dataset. Since all stitched images are of different sizes, the first augmentation must standardize the size. The result was very successful. Initially, a randomized crop of the desired size was used, since that allows for a significantly larger and more diverse dataset than a more traditional resize. However, results in figure 3 show that the model's training loss did not decrease with epochs, suggesting that the model is incapable of completing this task. All augmentations to the data were investigated, and the result was isolated to the random crop. A training loop with a resize transformation was used instead, shown in figure 4, and the training loss did decrease, though over training was a significant issue.  This implies that the relationship of far away pixels is important for performing this task. Self-attention layers are known for their ability to learn global information, so AttentionUnet, a unet architecture which incorporates self-attention layers in skip connections was used, and it significantly outperformed the unet model with the same number of epochs and ultimately converged to a significantly lower training loss. Additionally, the wide interquartile range in both figure 3 and 4 was caused by a single mislabeled image. A well optimized training loop with the mislabeled data point corrected is shown in figure 5 and 6 for u-net and attention unet respectively.



```{r resize,out.width="100%"}
#| echo: false
#| fig-cap: "Model Training"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/model_training.png")
```



While this model could have been used, this training process was not reproducible, so this was not pursued further.

### Segmenting Entire Surface

While we can segment the entire fatigue region, we need to account for the different magnification used during the imaging process. Instead of directly comparing the amount of pixels in the fatigue region between different samples, we can find the ratio of pixels in the fatigue region to the entire surface and use that ratio to compare different samples. This was done in the SAM_entire_surface.py script, which is in the appendix.

## Initiating Defect

### Inability to Automatically Segment



```{r X-AnyLabeling Figure,out.width="100%"}
#| echo: false
#| fig-cap: "Example Shown for SAM Model"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/SAM_webpage.png")
```



Figure 7 shows the segment anything model being used through it's webpage to segment an example initiating defect image. Based on this result, the surface can be segmented with a square of centralized points and the initiating defect is the largest defect inside of this surface. However, this is a high resolution image with good contrast between the surface, the foreground and the defect. An attempt was made to do this automatically, which was ultimately unsuccessful, so all images were labeled manually. The segmented results are saved as polygons whose points are in the points column of the dataframe.

### Supervised Segmentation



```{r SAM works figure,out.width="100%"}
#| echo: false
#| fig-cap: "Example Use of X-AnyLabeling"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/X-AnyLabeling.png")
```



Segmentation of the initiating defect was performed with X-AnyLabeling, an example of which is shown above. However, not all of the samples were able to be successfully segmented. Based on the figures below, these do not seem to be correlated to the process parameters, which is surprising, since the lack of fusion defects are substantially more complex than the keyhole or gas pores.



```{python extacting initiating defect features,echo = FALSE,error=FALSE,results='hide'}
x_SAM_process = []
y_SAM_process = []
x_SAM_test = []
y_SAM_test = []
x_not_SAM_process = []
y_not_SAM_process = []
x_not_SAM_test = []
y_not_SAM_test = []
for group_string, group in combined_df.groupby('sample_id'):
    no_points = group[(group['image_class']=='initiation') & (group['points'].isna())]
    points = group[(group['image_class']=='initiation') & (~group['points'].isna())]
    if len(no_points.index)>=1:
        x_not_SAM_process.append(no_points['scan_velocity_mm_s'].iloc[0])
        y_not_SAM_process.append(no_points['scan_power_W'].iloc[0])
        x_not_SAM_test.append(no_points['cycles'].iloc[0])
        y_not_SAM_test.append(no_points['test_stress_Mpa'].iloc[0])
    elif  len(points.index)>=1:
        x_SAM_process.append(points['scan_velocity_mm_s'].iloc[0])
        y_SAM_process.append(points['scan_power_W'].iloc[0])
        x_SAM_test.append(points['cycles'].iloc[0])
        y_SAM_test.append(points['test_stress_Mpa'].iloc[0])

def jitter(arr, jitter_amount=2):
    return arr + np.random.uniform(-jitter_amount, jitter_amount, len(arr))
  
success_scatter_fig,(SN_axs,PV_axs) = plt.subplots(2,1,figsize=(14,10))
plt.figure(figsize=(14, 10))

# Process Variables Plot
SN_axs.scatter(jitter(x_not_SAM_process), jitter(y_not_SAM_process), color='red', label='No Points', alpha=0.7)
SN_axs.scatter(jitter(x_SAM_process), jitter(y_SAM_process), color='blue', label='With Points', alpha=0.7)
SN_axs.set_xlabel('Scan Velocity (mm/s)',fontsize=16)
SN_axs.set_ylabel('Scan Power (W)',fontsize=16)
SN_axs.set_title('Process Variables',fontsize=24)
SN_axs.legend(fontsize=24)

# Test Variables Plot
PV_axs.scatter(jitter(x_not_SAM_test), jitter(y_not_SAM_test), color='red', label='No Points', alpha=0.7)
PV_axs.scatter(jitter(x_SAM_test), jitter(y_SAM_test), color='blue', label='With Points', alpha=0.7)
PV_axs.set_xlabel('Cycles',fontsize=16)
PV_axs.set_ylabel('Test Stress (MPa)',fontsize=16)
PV_axs.set_title('Test Variables',fontsize=24)
PV_axs.legend(fontsize=16)
success_scatter_fig.tight_layout()
success_scatter_fig.show()
```

```{python SAM success vs Process Parameters,echo = FALSE,error=FALSE,results='hide'}
SAM_process = []
SAM_test = []
not_SAM_process = []
not_SAM_test = []
for group_string, group in combined_df.groupby('sample_id'):
    no_points = group[(group['image_class']=='initiation') & (group['points'].isna())]
    points = group[(group['image_class']=='initiation') & (~group['points'].isna())]
    if not (len(no_points.index)>=1 and len(points.index)>=1):
        if len(no_points.index)>=1:
            not_SAM_process.append(no_points['energy_density_J_mm3'].iloc[0])
        elif  len(points.index)>=1:
            SAM_process.append(points['energy_density_J_mm3'].iloc[0])
plt.rcParams.update({'font.size': 32})  # Set font size for all elements
plt.figure(figsize=(10, 5))

# Process Variables Histograms
plt.hist(x_not_SAM_process, bins='auto', color='red', alpha=0.7, label='No Points')
plt.hist(x_SAM_process, bins='auto', color='blue', alpha=0.7, label='With Points')
plt.xlabel('Energy Density [J/mm^3]',fontsize=16)
plt.ylabel('Frequency',fontsize=16)
plt.title('Process Variables',fontsize=24)
plt.legend(fontsize=16)
plt.show()

x_not_SAM_process = np.array(x_not_SAM_process)
x_SAM_process = np.array(x_SAM_process)
x_not_SAM_process = x_not_SAM_process[~np.isnan(x_not_SAM_process)]
x_SAM_process = x_SAM_process[~np.isnan(x_SAM_process)]

statistic, p_value = scipy.stats.ks_2samp(np.array(x_not_SAM_process),np.array(x_SAM_process))
```

```{python SAM success vs Process Parameters hypothesis test,echo=FALSE}
print("P value: "+str(p_value))
if p_value < 0.05:
    print("Reject the null hypothesis: distributions are different")
else:
    print("Fail to reject the null hypothesis: distributions are the same")
```



The hypothesis test is narrowly failed, so the success and failure rate distributions accross the processing parameters are not significantly different.



```{python SAM success vs testing variables,echo = FALSE,error=FALSE,results='hide'}
x_SAM_test = []
x_not_SAM_test = []
for group_string, group in combined_df.groupby('sample_id'):
    no_points = group[(group['image_class']=='initiation') & (group['points'].isna())]
    points = group[(group['image_class']=='initiation') & (~group['points'].isna())]
    if not (len(no_points.index)>=1 and len(points.index)>=1):
        if len(no_points.index)>=1:
            cycles =no_points['cycles'].iloc[0]
            stress =no_points['test_stress_Mpa'].iloc[0]
            x_not_SAM_test.append(math.log(cycles)*math.log(stress))
        elif  len(points.index)>=1:
            cycles =points['cycles'].iloc[0]
            stress =points['test_stress_Mpa'].iloc[0]
            x_SAM_test.append(math.log(cycles)*math.log(stress))
plt.rcParams.update({'font.size': 32})  # Set font size for all elements
plt.figure(figsize=(10, 5))

# Process Variables Histograms
plt.subplot(1, 2, 1)
plt.hist(x_not_SAM_test, bins='auto', color='red', alpha=0.7, label='No Points')
plt.hist(x_SAM_test, bins='auto', color='blue', alpha=0.7, label='With Points')
plt.xlabel('log(cycles)*los(stress [Mpa])',fontsize=16)
plt.ylabel('Frequency',fontsize=16)
plt.title('Testing Variables',fontsize=20)
plt.legend(fontsize=16)
plt.show()

x_not_SAM_test = np.array(x_not_SAM_test)
x_SAM_test = np.array(x_SAM_test)
x_not_SAM_test = x_not_SAM_test[~np.isnan(x_not_SAM_test)]
x_SAM_test = x_SAM_test[~np.isnan(x_SAM_test)]

statistic, p_value = scipy.stats.ks_2samp(np.array(x_not_SAM_test),np.array(x_SAM_test))
```

```{python SAM success vs testing hypothesis test,echo=FALSE}
print("P value: "+str(p_value))
if p_value < 0.05:
    print("Reject the null hypothesis: distributions are different")
else:
    print("Fail to reject the null hypothesis: distributions are the same")
```



The hypothesis test is again narrowly failed, so the success and failure rate distributions accross the testing conditions are not significantly different.

### Evaluating Features



```{r Metrics explination figure,out.width="100%"}
#| echo: false
#| fig-cap: "Sharpness Figure"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/Sharpness_fig.png")
```



From knowledge of fracture mechanics, we know there is a link between the shape of the initiating defect and the resulting fatigue properties, so it should be possible to extract features from these images which predict fatigue performance. The most common feature is the aspect ratio. However, as shown in figure 8, shape 1 and 3 have similar aspect ratio, but shape 1 should create a significantly worse concentrating factor because of the sharp edge in the bottom right corner. Sharpness measures the change in radial distance from the centroid, which better captures these sharp edges, as can be seen by the high value for both shape 1 and 2. 



```{python initiating_defect_features,echo=FALSE,error=FALSE}
combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
points_df = combined_df[~combined_df['points'].isna()]
print(points_df['image_class'].value_counts())
print(points_df['sample_id'].value_counts().head(10))
df = initiating_defect_features.make_feature_df(points_df)
columns = ["screen_portion","max_sharpness","aspect_ratio","perimeter","pixels"]
df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)
df = df.dropna()
results_df = initiating_defect_features.regression_on_df(df)
print(results_df.loc[results_df["r2"].idxmax()])
print(results_df.loc[results_df[(results_df['aspect_ratio']==False) &(results_df['sharpness']==False)]["r2"].idxmax()])    
initiating_defect_features.plot_feature_df(df[columns])
```



From the above correlation plot, sharpness and aspect ratio do vary based on the portion of the screen the defect takes up. This means the measurement is independent of imaging magnification. From the regression information, we can see that the best regression was a polynomial regression with all processing parameters and the sharpness, with and $R^2=0.465$, with the best regression excluding extracted features having an $R^2=0.441$. This implies that the extracted features can marginally improve prediction of mechanical properties.



```{python initiating_defect_mask_validation,echo=FALSE,error=FALSE,results='hide'}
combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
points_df = combined_df[~combined_df['points'].isna()]
df = initiating_defect_features.make_feature_df(points_df)
columns = ["screen_portion","max_sharpness","aspect_ratio","perimeter","pixels"]
df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)
df = df.dropna()
# path = "/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/sam"
# # sam_checkpoint = path +"/sam_vit_h_4b8939.pth"
# url = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
# sam_checkpoint = urllib.request.urlretrieve(url)

model_type = "vit_h"

sam = sam_model_registry[model_type](checkpoint="/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/fractography_scripts/sam_vit_h_4b8939.pth")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Only move `sam` to GPU if an NVIDIA GPU is available
if torch.cuda.is_available():
    sam.to(device=device)
    print("Model moved to GPU.")
else:
    print("No NVIDIA GPU detected. Using CPU.")
SAM = SamPredictor(sam)

np.random.seed(3)

df['xy'] = df['imgs'].apply(initiating_defect_mask_validation.find_centroid).apply(np.array)
best_rows = []


df['SAM_raw_output'] =df.apply(lambda x: initiating_defect_mask_validation.process_row(x['imgs'],x['xy'],SAM),axis=1)
df['SAM_processed_output'] = df['SAM_raw_output'].apply(initiating_defect_mask_validation.process_mask).apply(initiating_defect_mask_validation.invert_mask)
df["cross_entropy"] = df.apply(
    lambda x: torch.nn.functional.binary_cross_entropy(
        torch.Tensor(cv2.resize(x['imgs'],(1024,1024))/255),
        torch.Tensor(x['SAM_processed_output']/255)
    )
,axis=1).apply(lambda x: x.detach().item())

for group_string, group in df.groupby(by="sample_id"):
    # print(group_string+" running")
    # group['SAM_output'] =group.apply(process_row,axis=1).apply(process_mask)
    # group["cross_entropy"] = group.apply(
    #     lambda x: torch.nn.functional.binary_cross_entropy(
    #         torch.Tensor(cv2.resize(x['imgs'],(1024,1024))/255),
    #         torch.Tensor(x['SAM_output']/255)
    #     )
    # ,axis=1)
    best_rows.append(
        group.loc[group['cross_entropy'].idxmin()]
    )
hist_fig, hist_ax = plt.subplots(1, 1, tight_layout=True)
hist_ax.hist(df['cross_entropy'],bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])
hist_fig.show()
```



From the above graphic, we can see that the majority of samples are high quality.



```{python portion vs entropy, echo=FALSE}
pve_fig, (pve_ax,good_pve_ax,bad_pve_ax) = plt.subplots(3, 1, tight_layout=True,figsize=(16,20))
pve_scatter = pve_ax.scatter(df['screen_portion'],df['cross_entropy'],
    c=df['energy_density'],
    cmap='inferno',  # or another colormap you prefer
    vmin=df['energy_density'].min(),
    vmax=df['energy_density'].max()/2)
pve_ax.set_xlabel("Portion of Screen")
pve_ax.set_ylabel("Binary Cross Entropy")
print(df['cross_entropy'].apply(lambda x:x<2).value_counts().rename("Enrtopy<2"))
good_df = df[df['cross_entropy'].apply(lambda x:x<2)].copy()
good_pve_scatter = good_pve_ax.scatter(good_df['screen_portion'],good_df['cross_entropy'],
    c=good_df['energy_density'],
    cmap='inferno',  # or another colormap you prefer
    vmin=df['energy_density'].min(),
    vmax=df['energy_density'].max()/2)
good_pve_ax.set_xlabel("Portion of Screen")
good_pve_ax.set_ylabel("Binary Cross Entropy")
# Compute a linear fit using numpy's polyfit
good_x = good_df['screen_portion'].values
good_y = good_df['cross_entropy'].values
good_m, good_b = np.polyfit(good_x, good_y, 1)
good_y_pred = good_m * good_x + good_b
good_y_mean = np.mean(good_y)
good_ss_tot = np.sum((good_y - good_y_mean)**2)
good_ss_res = np.sum((good_y - good_y_pred)**2)
good_r_squared = 1 - (good_ss_res / good_ss_tot)
# Plot the linear regression line
good_pve_ax.plot(good_x, good_y_pred, color='red', label=f'{round(good_m,1)}*x+{round(good_b,1)}, R^2={round(good_r_squared,3)}')
bad_df = df[df['cross_entropy'].apply(lambda x:x>2)].copy()
bad_pve_scatter = bad_pve_ax.scatter(bad_df['screen_portion'],bad_df['cross_entropy'],
    c=bad_df['energy_density'],
    cmap='inferno',  # or another colormap you prefer
    vmin=df['energy_density'].min(),
    vmax=df['energy_density'].max()/2)
bad_pve_ax.set_xlabel("Portion of Screen")
bad_pve_ax.set_ylabel("Binary Cross Entropy")
# Compute a linear fit using numpy's polyfit
bad_x = bad_df['screen_portion'].values
bad_y = bad_df['cross_entropy'].values
bad_m, bad_b = np.polyfit(bad_x, bad_y, 1)
bad_y_pred = bad_m * bad_x + bad_b
bad_y_mean = np.mean(bad_y)
bad_ss_tot = np.sum((bad_y - bad_y_mean)**2)
bad_ss_res = np.sum((bad_y - bad_y_pred)**2)
bad_r_squared = 1 - (bad_ss_res / bad_ss_tot)
bad_m, bad_b = np.polyfit(bad_df['screen_portion'], bad_df['cross_entropy'], 1)
bad_pve_ax.plot(bad_x, bad_y_pred, color='red', label=f'{round(bad_m,1)}*x+{round(bad_b,1)},R^2={round(bad_r_squared,3)}')
bad_pve_ax.legend()
pve_fig.colorbar(bad_pve_scatter, ax=bad_pve_ax, label=energy_density_label)
good_pve_ax.legend()
pve_fig.colorbar(good_pve_scatter, ax=good_pve_ax, label=energy_density_label)
good_pve_ax.legend()
pve_ax.plot(good_x, good_y_pred, color='red', label=f'{round(good_m,1)}*x+{round(good_b,1)},R^2={round(good_r_squared,3)}')
pve_ax.plot(bad_x, bad_y_pred, color='red', label=f'{round(bad_m,1)}*x+{round(bad_b,1)},R^2={round(bad_r_squared,3)}')
pve_ax.legend()
pve_fig.colorbar(pve_scatter, ax=pve_ax, label=energy_density_label)
pve_fig.show()
```



During segmentation, it was observed that there seemed to be an ideal size where the model could more easily segment the defects. This can be seen in the above graph where in both low and high quality samples, the larger the sample on the screen the less agreement it had with the existing mask.



```{python energy density vs entropy, echo=FALSE}
entropy_vs_energy_fig, entropy_vs_energy_ax = plt.subplots(1, 1, tight_layout=True)
entropy_vs_energy_scatter = entropy_vs_energy_ax.scatter(df['energy_density'],df['cross_entropy'],
    c=df['energy_density'],
    cmap='inferno',  # or another colormap you prefer
    vmin=df['energy_density'].min(),
    vmax=df['energy_density'].max()/2)
entropy_vs_energy_ax.set_xlabel("Energy Density [J/mm^3]")
entropy_vs_energy_ax.set_ylabel("Binary Cross Entropy")
entropy_vs_energy_fig.colorbar(entropy_vs_energy_scatter, ax=entropy_vs_energy_ax, label=energy_density_label)
entropy_vs_energy_fig.show()
```



We can see that the model performs well.

# Discussion

The primary result of this investigation is that features from the initiating defect of the fatigue fracture surface can be predictive of cycles to failure. Additionally, segmenting these images would have been in-feasible without the aid of the segment anything model. We can also see that aspect ratio does not change with the size of the image, which means it can be applied to any image resolution. Additionally, the attempt to segment the fatigue region shows that global context can be important for mechanical properties.

# Conclusion

This project showed that prediction of fatigue performance can be improved with the use of features extracted from segmenting the initiating defect, and that modern tools make this far easier than in the past. This conclusion was reach by:

1. Curating a tidy dataframe from the unstructured image folders and excel sheets
2. Using modern AI enhanced segmentation tools to segment all initiating defects
3. Cleaning these masks
4. Extracting features from these masks
5. Validating these features to ensure that they are independent of imaging parameters
6. Performing regression with these images

# Citations

::: {#refs}
:::

# Appendix

## organize_data.py



```{python, eval = FALSE}
# %% Import
import pandas as pd
import re
import datetime
import cv2
import numpy as np
import math
import os
import PIL
import numpy as np
import concurrent.futures as futures

STITCHED_THRESHOLD = 3000000
UPPER_RED_THRESHOLD = 1000
LOWER_RED_THRESHOLD = 20
HATCH_SPACING = 0.14 #mm
LAYER_THICKNESS = 0.03 #mm

LOG_FILE = '/home/aml334/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df_log.txt'
MESSAGE = 'Specified joins because only images with points were being kept in the csv'

manuel_mask_path = '/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/keyence-fractography/manuel_mask'
fractography_path = '/mnt/vstor/CSE_MSE_RXF131/staging/mds3/fractography'
polygons_path = '/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/shape_infos.csv'
path_list = [fractography_path,manuel_mask_path]
pd.set_option('display.max_rows', None)  # Display all rows

check = re.compile(r'''
    ^
    (?:x)?
    (?:\d+)?
    (?:[a-b])?
    [-]?
    (?:Copy\ of\ |Overview|_STD_ETD_|Initiation)?  # Optional prefixes
    [-]?
    [\d]?
    [-]?
    [x]?
    (?:\d+)?
    [-]?
    (EP|NASA|CMU)                                  # Start with EP, NASA, or CMU (case insensitive)
    [-_]?                                          # Optional separator
    (\d+|O\d+)                                     # Number or O followed by number
    [-_]?                                        # Optional separator
    V?
    ([E\d]+|\d+)                                   # Version number
    [-_]?                                          # Optional separator
    (\d+)?                                         # Optional additional number
    (?:_MARKED)?                                   # Optional '_MARKED' suffix
    (.*)?                                          # Any characters in between (greedy by default)
    \.(png|tif|tiff|jpg)$                          # File extension
    ''', re.VERBOSE | re.IGNORECASE)

def log_message(file_path,message):
    log_entry = f'[{datetime.datetime}] {message}\n'

    # Append the log entry to the file
    with open(file_path, 'a') as file:
        file.write(log_entry)
def standardize_sample_num(x):
    try:
        m_f = re.match(r'^([A-Z]+)(\d+)-[V]?(\d+|E\d+)[-]?(\d+)?',x)
    except TypeError:
        return None
    if m_f:
        if(m_f.lastindex==4):
            return m_f.group(1)+m_f.group(2).lstrip('0').lstrip('O')+'-'+m_f.group(3)+'-'+str(int(m_f.group(4)))
        else:
            return m_f.group(1)+m_f.group(2).lstrip('0').lstrip('O')+'-'+m_f.group(3)+'-'+str('1')
    else:
        return None
def name_to_power(name:str, position:int):
    for idx, option in enumerate(process_parameters['Test ID']):
        if(option[2] == name[position]):
            return process_parameters['P (W)'][idx]
def name_to_velocity(name:str,position:int):
    for idx, option in enumerate(process_parameters['Test ID']):
        if(option[2] == name[position]):
            return process_parameters['V (mm/s)'][idx]
def clean_name(input):
    return input.astype(str).str.replace('0','').str.rstrip('.')
def clean_BuildID(input):
    try:
        match = re.match(r'([A-Z]+)(\d+)',input,re.IGNORECASE)
        if match:
            prefix = match.group(1).upper().lstrip('O')
            numeric_part = match.group(2).lstrip('0')
            return prefix + numeric_part
    except TypeError:
        print(input)
#Adds File if Condition(path) return True
def recursive_search(condition,path:str, file_list:list):
    if os.path.isdir(path):
        for path_loop in os.listdir(path):
            recursive_search(condition,os.path.join(path,path_loop),file_list)
    else:
        if(condition(path)):
            file_list.append(path)
    return file_list
#Functions used for Validation
def size(image_path):
    img = cv2.imread(image_path)
    try:
        return img.shape[0] * img.shape[1]
    except AttributeError:
        print('File is corrupted: '+image_path)
        return -1
def size_red(image_path):
   img = cv2.imread(image_path)
   try:
        #Convert the image from BGR to HSV color space
        hsv_image = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

        # Define the lower and upper bounds for the red color in HSV space
        lower_red_1 = np.array([0, 50, 50])
        upper_red_1 = np.array([10, 255, 255])
        lower_red_2 = np.array([170, 50, 50])
        upper_red_2 = np.array([180, 255, 255])

        # Create masks for the red color ranges
        mask1 = cv2.inRange(hsv_image, lower_red_1, upper_red_1)
        mask2 = cv2.inRange(hsv_image, lower_red_2, upper_red_2)

        # Combine the masks
        red_mask = mask1 + mask2

        # Count the number of red pixels
        red_pixels = cv2.countNonZero(red_mask)

        return(red_pixels)
   except AttributeError:
       print('File is corrupted: '+image_path)
       return -1
def is_greyscale(image_path):
    img = cv2.imread(image_path)
    if len(img.shape) < 3:
        return True
    if img.shape[2] == 1:
        return True
    # If the image is color, check if all channels are equal
    if np.allclose(img[:, :, 0], img[:, :, 1]) and np.allclose(img[:, :, 1], img[:, :, 2]):
        return True
    return False

def is_binary(path):
    try:
        img = PIL.Image.open(path)
        img = img.convert('L')
        img_data = np.array(img)
        unique_vals = np.unique(img_data)
        if len(unique_vals) == 2 and set(unique_vals) == {0, 255}:
            return True
        else:
            return False
    except PIL.UnidentifiedImageError:
        return False    

def is_8bit(path):
    try:
        img = PIL.Image.open(path)
        img = img.convert('L')
        img_data = np.array(img)
        if img_data.min() < 0 or img_data.max() > 255:
            return False
        else:
            return True
    except PIL.UnidentifiedImageError:
        return False

#Different columns that would be valuable to have
def valid_image(path):
    for i in ['.csv','.hdr','.xlsx','Contaminated']:
        if i in path:
            return False
    if is_8bit(path):
        return True
    else:
        return False
def marked(path):
    if 'marked' in path.lower() and valid_image(path) and not is_greyscale(path):
        return True
    else:
        return False
def initiation(path):
    if ('_001' in path or 'initiation' in path.lower()) and valid_image(path) and size(path)<STITCHED_THRESHOLD:
        return True
    else:
        return False
def stitched(path):
    if ('stitched' in path.lower() or 'composite' in path.lower()) and (not 'weka' in path) and valid_image(path) and size(path) >STITCHED_THRESHOLD:
        return True
    else:
        return False
def full_surface_marked(path):
    if stitched(path) and (not is_greyscale(path)) and (size_red(path)>UPPER_RED_THRESHOLD):
        return True
    else:
        return False
def initation_marked_stitched(path):
    if stitched(path) and ('MARKED' in path) and (not is_greyscale(path)) and (LOWER_RED_THRESHOLD<size_red(path)<UPPER_RED_THRESHOLD):
        return True
    else:
        return False
def fatigue(path):
    if 'fatigue' in path.lower() and valid_image(path) and is_binary(path):
        return True
    else:
        return False
def overload(path):
    if 'overload' in path.lower() and valid_image(path) and is_binary(path):
        return True
    else:
        return False
def full_surface_unmarked(path):
    if stitched(path) and (not 'marked' in path.lower()) and (not is_binary(path)):
        return True
    else:
        return False
def exclude(input):
    conditions = ['.hdr', '.csv','.info','.xlsx','.info','.pptx','.s0001','.zip','.model']
    for condition in conditions:
        if condition in input: return True
    return False

def check_regex_basename(dict_to_search,search_function, exclude_conditions):
    i=0
    for key in dict_to_search:
        for field in dict_to_search[key]:
            basename = field.split('/')[-1]
            if not search_function(basename) and not exclude(basename):
                print(basename)
                i+=1
        print('Unselected files: '+str(i))
def regex_basename(pattern):
    match = re.search(check,pattern)
    if(match):
        type_func = clean_BuildID(match.group(1)+match.group(2))
        series_func = match.group(3).lstrip("0")
        if match.group(4):
            posit_idx_func = match.group(4).lstrip("0")
        else:
            posit_idx_func = None
        return type_func, series_func,posit_idx_func
    else:
        return None
condition_list = [
    valid_image,
    initiation,
    stitched,
    full_surface_marked,
    initation_marked_stitched,
    fatigue,
    overload,
    full_surface_unmarked
]
def print_column_counts(df,example=0):
    row_structure = '|{:^25}|{:^10}|{:^10}|{:^15}|{:^15}|'
    print(row_structure.format('Column name', 'Nulls','Values','dtype','Example'))
    for column in df.columns:
        nas = df[column].isna().sum()
        col_type = df[column].dtype
        print(row_structure.format(
            column,
            str(nas),
            str(len(df[column])-nas),
            str(col_type),
            str(df[column].iloc[example])[0:15],
            ))
# %% 
'''Tidy EP and NADA data'''
if __name__=="__main__":
    # EP04,5,7 + NASA
    EP04 = pd.read_excel('/mnt/vstor/CSE_MSE_RXF131/staging/mds3/fractography/EP04 (Complete)/EP04 Fractographical Data.xlsx')
    EP05 = pd.read_excel('/mnt/vstor/CSE_MSE_RXF131/staging/mds3/fractography/EP05/EP05 Fractographical Data_cycles_added.xlsx')
    EP07 = pd.read_excel('/mnt/vstor/CSE_MSE_RXF131/staging/mds3/fractography/EP07/EP07-Fractographical Data.xlsx')
    NASA = pd.read_excel('/home/aml334/CSE_MSE_RXF131/staging/mds3/fractography/NASA03/NASA Fractographical Data_Chris-Updated 9_2.xlsx',skiprows=1)
    EP_NASA_df = pd.concat([EP04,EP05,EP07,NASA])
    EP_NASA_df['Sample#'] = EP_NASA_df['Sample#'].apply(standardize_sample_num)
    del EP04
    del EP05
    del EP07
    del NASA
    EP_NASA_df = EP_NASA_df[['Sample#','σ (Mpa)','Cycles']]
    EP_NASA_df = EP_NASA_df.dropna()
    EP_NASA_df = EP_NASA_df.rename(columns={
        'Sample#':'sample_id',
        'σ (Mpa)':'test_stress_Mpa',
        'Cycles':'cycles'
    })
    EP_NASA_df = EP_NASA_df.astype({
        'sample_id':'string',
        'test_stress_Mpa':'float',
        'cycles':'int32'
    })
    # Needs to be done later
    process_parameters = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/staging/mds3/fractography/variable-process-parameters.csv')
    EP_NASA_df['scan_power_W'] = EP_NASA_df['sample_id'].apply(lambda row:name_to_power(row,4)).astype('float')
    EP_NASA_df['scan_velocity_mm_s'] = EP_NASA_df['sample_id'].apply(lambda row: name_to_velocity(row,4)).astype('float')
    EP_NASA_df['energy_density_J_mm3'] = EP_NASA_df['scan_power_W']/(EP_NASA_df['scan_velocity_mm_s'].apply(lambda x: x*HATCH_SPACING*LAYER_THICKNESS))
    EP_NASA_df['build_id'] = EP_NASA_df['sample_id'].str.split("-").apply(lambda x: x[0])
    EP_NASA_df['build_plate_position'] = EP_NASA_df['sample_id'].str.split("-").apply(lambda x: x[1])
    EP_NASA_df['testing_position'] = EP_NASA_df['sample_id'].str.split("-").apply(lambda x: x[2])
    del process_parameters 

    # %%
    '''Tidying Brett's dataframe'''
    #Brett Spreashsheet
    Brett_spreadsheet = pd.ExcelFile('/mnt/vstor/CSE_MSE_RXF131/staging/mds3/fractography/4-pt Bend Data Master Spreadsheet_exit_8_27_24.xlsx')
    Brett_df = pd.DataFrame()
    for worksheet in Brett_spreadsheet.sheet_names:
        if worksheet not in ['Template','To Test','Retest']:
            Brett_df = pd.concat([Brett_df,pd.read_excel(Brett_spreadsheet,worksheet)])
    del Brett_spreadsheet
    Brett_df = Brett_df[['Build ID','Build #','Test #','Scan Power (W)','Scan velocity (mm/s)','Retest','σ max initiation (MPa)','Cycles']]
    Brett_df = Brett_df.rename(columns={
        'Build ID':'build_id',
        'Build #':'build_plate_position',
        'Test #':'testing_position',
        'Scan Power (W)': 'scan_power_W',
        'Scan velocity (mm/s)':'scan_velocity_mm_s',
        'σ max initiation (MPa)':'test_stress_Mpa',
        'Cycles':'cycles',
        })
    #Filtering out NA values
    Brett_df['testing_position'] = pd.to_numeric(Brett_df['testing_position'], errors='coerce')
    Brett_df = Brett_df.dropna()
    Brett_df = Brett_df.astype({
        'build_id':'string',
        'build_plate_position':'string',
        'testing_position':'int32',
        'test_stress_Mpa':'float',
        'cycles':'int32'
    })
    Brett_df['build_id'] = Brett_df['build_id'].apply(clean_BuildID)
    Brett_df['sample_id'] = Brett_df['build_id'] + '-'+Brett_df['build_plate_position'].apply(str).apply(lambda x:x.replace('V','').replace('.0','')).replace('O','')+'-'+Brett_df['testing_position'].apply(str).apply(lambda x:x.replace('V','').replace('.0',''))
    #Filtering out mechanical data from times the sample didn' break (we get no metallography data)
    Brett_df = Brett_df.reset_index(drop=True)
    Brett_df = Brett_df.iloc[Brett_df.groupby('sample_id')['Retest'].idxmax()]
    Brett_df = Brett_df.drop(columns=['Retest'])
    Brett_df['energy_density_J_mm3'] = Brett_df['scan_power_W']/(Brett_df['scan_velocity_mm_s'].apply(lambda x: x*HATCH_SPACING*LAYER_THICKNESS))

    # %%
    '''Tidying Austin's Data'''
    #Austin's spreedsheet
    Austin_spreadsheet = pd.ExcelFile('/home/aml334/CSE_MSE_RXF131/staging/mds3/fractography/MasterSheet_ULI_Ti6Al4V_Fatigue.xlsx')
    Austin_df = pd.DataFrame()
    for worksheet in ['Fatigue Test Table','K calculation']:
        if 'Fatigue Test Table' in worksheet:
            x = pd.read_excel(Austin_spreadsheet,worksheet,skiprows=1)
            x['cycles'] = x['Cycles @ Failure']
            x['test_stress_Mpa'] = x['MPa']

        elif 'K calculation' in  worksheet:
            x = pd.read_excel(Austin_spreadsheet,worksheet,skiprows=0)
            x['test_stress_Mpa'] = x['Mpa']
        x = x.loc[:, ~x.columns.str.startswith('Unnamed')]
        Austin_df = pd.concat([Austin_df,x])
    del x
    del Austin_spreadsheet
    #Extracting infromation from spreadsheet
    Austin_df = Austin_df.reset_index(drop=True)
    ID_regex = re.compile(r"^(EP\d+|CMU\d+|NASA\d+)-(V?\d+)-(\d)",re.IGNORECASE)
    result = Austin_df.loc[~Austin_df['ID'].isna(), 'ID'].apply(lambda x: re.search(ID_regex, str(x)) if re.search(ID_regex, str(x)) else None)
    result = result[~result.isna()]
    Austin_df['build_id'] = result.apply(lambda x: clean_BuildID(x.group(1)))
    Austin_df['build_plate_position'] = result.apply(lambda x: x.group(2))
    Austin_df['testing_position'] = result.apply(lambda x: x.group(3))
    Austin_df = Austin_df[~Austin_df['build_id'].isna()]
    Austin_df['sample_id'] = Austin_df['build_id']+"-"+Austin_df['build_plate_position']+"-"+Austin_df["testing_position"]
    Austin_df = Austin_df[['build_id','build_plate_position','testing_position','sample_id','cycles','test_stress_Mpa']]
    Austin_df = Austin_df.dropna()
    Austin_df = Austin_df.astype({
        'build_id':'string',
        'build_plate_position':'string',
        'sample_id':'string',
        'testing_position':'int32',
        'test_stress_Mpa':'float',
        'cycles':'int32'
    })
    CMU_master_spreadsheet = pd.ExcelFile('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/Adcock_ULI_Build_Codex (1).xlsx')
    CMU_NASA_process_parameters_df = pd.DataFrame()
    for worksheet in ['CMU01','CMU02','CMU03','CMU04','CMU05','CMU07','CMU08','CMU09','CMU10','CMU11','CMU12']:
        x = pd.read_excel(CMU_master_spreadsheet,worksheet,skiprows=23)
        x = x[['Unnamed: 0','power','velocity']]
        x = x.rename(columns={
            'Unnamed: 0':'build_plate_position',
            'power': 'scan_power_W',
            'velocity': 'scan_velocity_mm_s'
        })
        x['energy_density_J_mm3'] = x['scan_power_W']/(x['scan_velocity_mm_s'].apply(lambda x: x*HATCH_SPACING*LAYER_THICKNESS))
        x['build_id'] = clean_BuildID(worksheet)
        x = x.dropna().reset_index(drop=True)
        x['build_plate_position'] = x['build_plate_position'].astype(np.int64)
        CMU_NASA_process_parameters_df = pd.concat([CMU_NASA_process_parameters_df,x])
    CMU_NASA_process_parameters_df=CMU_NASA_process_parameters_df.dropna().reset_index(drop=True)
    CMU_NASA_process_parameters_df = CMU_NASA_process_parameters_df.astype({
            'build_id':'string',
            'build_plate_position':'string',
            'scan_power_W':'float',
            'scan_velocity_mm_s':'float',
            'energy_density_J_mm3':'float',
        })
    Austin_df = pd.merge(Austin_df,CMU_NASA_process_parameters_df,on=['build_id','build_plate_position'])
    df = pd.concat([Austin_df,Brett_df,EP_NASA_df]).drop_duplicates()
    df = df.astype({
        'sample_id':'string',
        'build_id':'string',
        'build_plate_position':'string',
        'testing_position':'int',
        'scan_power_W':'float',
        'energy_density_J_mm3':'float',
        'test_stress_Mpa':'float',
        'cycles':'int',
    })
    df['build_plate_position'] = df['build_plate_position'].str.removesuffix(".0")
    df=df[df['cycles']!=10000000]
    df = df.drop_duplicates().reset_index(drop=True)
    x = df['sample_id'].value_counts()
    x = x[x!=1].index
    for sample in x:
        df = df[df['sample_id'].apply(lambda y: not sample in y)].reset_index(drop=True)
    print('Mechanical Data')
    print_column_counts(df)
    # %%
    '''Extracting Image file List'''
    name = []
    column_dict = {}
    i = 0
    column_list = []

    def process_folder(column, top_folder):
        # Function to process each top_folder
        return recursive_search(column, top_folder, [])

    for column in condition_list:
        temp_list = []
        with futures.ThreadPoolExecutor() as executor:
            # Submit tasks for processing each folder in parallel
            results = executor.map(process_folder, [column]*len(path_list), path_list)
            
            # Combine results
            for result in results:
                temp_list.extend(result)
        
        # Store results
        name.append((column.__name__, len(temp_list)))
        column_dict[name[i][0]] = temp_list
        print(str(name[i]) + f'\tPosition: {i}')
        i += 1
    # %%
    '''Add Combine Dataframes''' 
    dataframe_list = []
    for i, key in enumerate(column_dict):
        build_num_column = []
        build_plate_position_column = []
        test_position_column = []
        basename = []
        Sample_num = []
        path_column = []
        for j, field in enumerate(column_dict[key]):
            if regex_basename(field.split('/')[-1]):
                path_column.append(field)
                type_inst, series_inst, posit_idx_inst = regex_basename(field.split('/')[-1])
                build_num_column.append(type_inst)
                build_plate_position_column.append(series_inst)
                test_position_column.append(posit_idx_inst)
                basename.append(field.split('/')[-1])
                if posit_idx_inst == None:
                    Sample_num.append(type_inst.upper()+'-'+str(series_inst)+'-1')
                else:
                    Sample_num.append(type_inst.upper()+'-'+str(series_inst)+'-'+str(posit_idx_inst))
        path_column = pd.Series(path_column,name='image_path')
        build_num_column = pd.Series(build_num_column, name='build_id')
        build_plate_position_column = pd.Series(build_plate_position_column,name='build_plate_position')
        test_position_column = pd.Series(test_position_column,name='testing_position')
        basename_column = pd.Series(basename,name='image_basename')
        Sample_num_column= pd.Series(Sample_num, name = 'sample_id')
        df_temp = pd.concat(
            [
                Sample_num_column,
                path_column,
                build_num_column,
                build_plate_position_column,
                test_position_column,
                basename_column,
            ],
            axis=1
        )
        df_temp['image_class'] = key
        dataframe_list.append(df_temp)
    df_polygons = pd.read_csv(polygons_path)[['Filename','Points']]
    df_polygons = df_polygons.rename(columns={
        'Filename':'image_basename',
        'Points':'points'
    })
    df_polygons['points'] = df_polygons['points'].apply(lambda x: x.lstrip("\""))
    df_imgs = pd.concat(dataframe_list, axis=0).merge(df_polygons,on='image_basename',how='left').reset_index(drop=True)
    df_final = pd.merge(df,df_imgs,on='sample_id',how='outer').reset_index(drop=True)
    # %%
    '''Save Files'''
    df_final.to_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
    log_message(LOG_FILE,str(len(df_final))+' :'+MESSAGE)
    row_structure = '|{:^50}|{:^10}|{:^10}|{:^15}|'
    print(row_structure.format('Column name', 'Nulls','Values','Position'))
    i=0
    for column in df_final.columns:
        nas = df_final[column].isna().sum()
        print(row_structure.format(column,str(nas),str(len(df_final[column])-nas),str(i)))
        log_message(LOG_FILE,str(row_structure.format(column,str(nas),str(len(df_final[column])-nas),str(i))))
        i+=1

else:
    print(__name__+' functions loaded')

```



## fatigue_training.py



```{python, eval = FALSE}
import argparse
parser = argparse.ArgumentParser(description = 'Parameters')
parser.add_argument("epochs",type=int)
parser.add_argument("accumulation_steps",type=int)
parser.add_argument("batch_size",type=int)
parser.add_argument("input_channels",type=int)
parser.add_argument("output_channels",type=int)
parser.add_argument("learning_rate",type=float)
parser.add_argument("imgs_per_transform",type=int)
parser.add_argument("path",type=str)
parser.add_argument('--local-rank', type=int, default=0, help="Local rank of the process for distributed training")
row_structure = '|{:^25}|{:^40}|'
epochs_print = '|{:^5}|{:^5}|{:^10}|{:^10}|{:^10}|'
args = parser.parse_args()
for arg, value in vars(args).items():
    print(row_structure.format(arg,str(value)[-40:]),flush=True)
print('-'*(25+40+3))
# %%
import pandas as pd
import os
import cv2
import time
import sys
import torch
import numpy
import matplotlib.pyplot as plt
import random
import torchvision.transforms.v2 as v2

start_time = time.perf_counter()
torch.manual_seed(9192024)
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ['TORCH_USE_CUDA_DSA'] = "1"
os.environ['TORCH_DISTRIBUTED_DEBUG'] = "INFO"
# os.environ['find_unused_parameters'] = "True"
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler

sys.path.append('/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/packages/AdvSegLearn/AdvSegLearn')
from unet import Unet
from semi_supervised_loss import semi_supervised_loss
from discriminator_loss import discriminator_loss
from multiclass_dataset import Multiclass_dataset
from train_GAN import train_GAN
from FCN import FCDiscriminator
sys.path.append('/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/Semantic-Segmentation-Architecture/PyTorch')
import attention_unet

print(torch.cuda.device_count())

def setup(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
df = combined_df[combined_df['image_basename'].apply(lambda x: type(x)==str)] #Drops nas, which are loaded as floats
df = df.groupby('sample_id')
x_sup = []
y_sup = []
for group_string, sample in df:
    if 'overload' in sample['image_class'].value_counts().index and 'full_surface_unmarked' in sample['image_class'].value_counts().index:
        y_sup.append(sample[(sample['image_class']=='overload') & (sample['image_path'].apply(lambda x: '.png' in x))]['image_path'].iloc[0])
        x_sup.append(sample[(sample['image_class']=='full_surface_unmarked')]['image_path'].iloc[0])
x_temp = pd.Series(x_sup,name='input')
y_temp = pd.Series(y_sup,name='output')
temp_df = pd.concat([x_temp,y_temp],axis=1)
temp_df.to_csv(args.path + '/dataset.csv')
TRAIN_SPLIT=0.8
VAL_SPLIT=0.2
split_idx=int(len(x_sup)*TRAIN_SPLIT-1)
print('Split idx: '+str(split_idx))
print('Data size: '+str(len(y_sup)))
x_sup_train = [x_sup[:split_idx]]
y_sup_train = [y_sup[:split_idx]]
x_sup_valid = [x_sup[split_idx:]]
y_sup_valid = [y_sup[split_idx:]]

blur_affine_trans = v2.Compose(
    [
        v2.RandomAffine(
            degrees=180,
            scale=[0.5,2],
            shear=[-15,-15,15,15]
        ),
        v2.GaussianBlur(
            [3,3],
        )
    ]
)
unblur_affine_trans = v2.Compose(
    [
        v2.RandomAffine(
            degrees=180,
            scale=[0.5,2],
            shear=[-15,-15,15,15]
        ),
    ]
)
Resize = v2.Resize([512,512],antialias=True)
train_ds = Multiclass_dataset(
    x_sup=x_sup_train,
    y = y_sup_train,
    initalization_transform=Resize,
    getitem_transform=unblur_affine_trans,
    imgs_per_transform=args.imgs_per_transform
)
valid_ds = Multiclass_dataset(
    x_sup=x_sup_valid,
    y = y_sup_valid,
    initalization_transform=Resize
)

rank = args.local_rank
world_size = int(os.environ['WORLD_SIZE'])
torch.cuda.set_device(rank)
setup(rank,world_size)
COMPLETE=False
BATCH_SIZE = args.batch_size
LEARNING_RATE = args.learning_rate * BATCH_SIZE * world_size
segmentor = attention_unet.attention_unet(dropout_prob=0.3).to(rank)
# Setting the dataset
segmentor = torch.nn.parallel.DistributedDataParallel(segmentor)
train_samp = DistributedSampler(train_ds,rank=rank,shuffle=True)
valid_samp = DistributedSampler(valid_ds,rank=rank,shuffle=False)
while not COMPLETE:
    try:
        train_dl = torch.utils.data.DataLoader(train_ds,sampler=train_samp,batch_size=BATCH_SIZE)
        valid_dl = torch.utils.data.DataLoader(valid_ds,sampler=valid_samp,batch_size=BATCH_SIZE)
        print('dataset finished loading')

        loss = torch.nn.BCELoss()
        optimizer = torch.optim.Adam(lr=LEARNING_RATE,params=segmentor.parameters())
        train_loss = []
        valid_loss = []
        epoch_times = []
        loading_time = time.perf_counter()
        print('Loading time: '+str(loading_time-start_time))
        print(epochs_print.format('Epoch','Rank','Time','Train Loss','Valid Loss',flush=True))
        for i in range(args.epochs):
            segmentor.train()
            temp = []
            only_one=True
            for x,y in train_dl:
                x, y = x.to(rank), y.to(rank)
                if only_one and i==0 and args.local_rank ==0:
                    expected_input = (torch.select(x,0,0).to('cpu').permute(1,2,0).detach().numpy()*255).astype(numpy.uint8)
                    cv2.imwrite(args.path+'/ex_train_x.png',expected_input)
                    expected_output = (torch.select(y,0,0).to('cpu').permute(1,2,0).detach().numpy()*255).astype(numpy.uint8)
                    cv2.imwrite(args.path+'/ex_train_y.png',expected_output)
                    del expected_input
                    del expected_output
                    only_one=False
                dist.barrier() #Make sure all of them are ready
                segmentor.zero_grad()
                s = segmentor(x)
                loss_point = loss(s,y)
                loss_point.backward()
                temp.append(loss_point.item())
                optimizer.step()
                optimizer.zero_grad()
            train_loss.append(train_GAN.metrics_list(temp))
            segmentor.eval()
            temp=[]
            only_one=True
            for x,y in valid_dl:
                x, y = x.to(rank), y.to(rank)
                if only_one and i==0 and args.local_rank ==0:
                    expected_input = (torch.select(x,0,0).to('cpu').permute(1,2,0).detach().numpy()*255).astype(numpy.uint8)
                    cv2.imwrite(args.path+'/ex_valid_x.png',expected_input)
                    expected_output = (torch.select(y,0,0).to('cpu').permute(1,2,0).detach().numpy()*255).astype(numpy.uint8)
                    cv2.imwrite(args.path+'/ex_valid_y.png',expected_output)
                    del expected_input
                    del expected_output
                    only_one=False
                s = segmentor(x)
                loss_point=loss(s,y)
                temp.append(loss_point.item())
            valid_loss.append(train_GAN.metrics_list(temp))
            epoch_time= time.perf_counter()
            if i==0:
                epoch_times.append(epoch_time - loading_time)
            else:
                epoch_times.append(epoch_time- sum(epoch_times) - loading_time)
            print(epochs_print.format(str(i),str(rank),str(epoch_times[i])[:10],str(train_loss[i][0])[:10],str(valid_loss[i][0])[:10]),flush=True)
            if args.local_rank==0:
                print('max: '+str(max(epoch_times)))
                print('average: '+str(sum(epoch_times)/len(epoch_times)))
                train_GAN.save_loss_plot(
                    loss_metrics = [
                        train_loss,
                        valid_loss
                    ],
                    legend_titles = [
                        'BCE training loss',
                        'BCE validation loss'
                    ],
                    order = [[0,1]],
                    save_path_fig = args.path+'/loss_figure.png',
                    save_path_csv = args.path+'/loss_figure.csv',
                    subplot_titles=['Loss Figure']
                )
                torch.save(segmentor,args.path+'/model_weights.pt')
        if args.local_rank==0:
            df = combined_df[combined_df['sample'].str.contains('CMU9') & -combined_df['path_stitched'].isna()]

            x_unsup=[]
            only_once=True
            for csv in df['path_stitched']:
                if(only_once):
                    temp = pd.read_csv(csv)
                    x_unsup.append(temp['path'].tolist()[0])
                    only_once = False
            x_unsup = [x_unsup]
            ds = Multiclass_dataset(x_unsup=x_unsup,initalization_transform=v2.Resize([512,512],antialias=True))
            only_once = True
            segmentor.eval()
            for x in ds:
                if only_once:
                    x = x.to(rank)
                    x = torch.unsqueeze(x,0)
                    output=segmentor(x)
                    x = torch.squeeze(x,0)
                    output = torch.squeeze(output,0)
                    output=(output.to('cpu').permute(1,2,0).detach().numpy()*255).astype(numpy.uint8)
                    expected_input = (x.to('cpu').permute(1,2,0).detach().numpy()*255).astype(numpy.uint8)
                    expected_input - cv2.cvtColor(expected_input,cv2.COLOR_RGB2BGR)
                    only_once=False
                    cv2.imwrite(args.path+'/ex_out.png',output,)
                    cv2.imwrite(args.path+'/ex_in.png',expected_input)
            COMPLETE=True
            print(row_structure.format('BATCH_SIZE',str(BATCH_SIZE)[-40:]),flush=True)
            print(row_structure.format('LEARNING_RATE',str(LEARNING_RATE)[-40:]),flush=True)
            print(row_structure.format('GPU VRAM size',str(torch.cuda.get_device_properties(torch.cuda.current_device()).total_memory)[-40:]),flush=True)
    except torch.cuda.OutOfMemoryError as e:
        BATCH_SIZE -=1
        LEARNING_RATE = args.learning_rate * BATCH_SIZE * world_size
        del x
        del y
        torch.cuda.empty_cache()
        print(f"Error occured\n{e}\nOOM error occured. Retrying with a smaller batch size of {BATCH_SIZE}")
        if BATCH_SIZE==0:
            print(f"GPU does not have enough memory to support this training scheme.")
            raise torch.cuda.OutOfMemoryError
dist.barrier() #Make sure all of them are ready
dist.destroy_process_group()
# %% Save Script
# Get the path of the current script
script_path = os.path.abspath(__file__)

# Open the script itself and read its contents
with open(script_path, 'r') as script_file:
    script_content = script_file.read()

# Define the path where you want to save the log (e.g., folder 'logs')
log_file_path = args.path + '/script_log.txt'

# Write the content of the script into the log file
with open(log_file_path, 'w') as log_file:
    log_file.write(script_content)

print(f"Script content has been logged to {log_file_path}")
print(1+'1')# Throwing error ends the script

```



## SAM_entire_surface.py



```{python, eval = FALSE}
# %%
import os
import sys
import numpy as np
import urllib.request
import cv2
import pandas as pd
import math
import numpy as np
import sys
import os
import random
import matplotlib.pyplot as plt
import torch
import organize_data
from segment_anything import sam_model_registry, SamPredictor

idx=0

def show_mask(mask, ax, random_color=False, borders = True):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask = mask.astype(np.uint8)
    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    if borders:
        import cv2
        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) 
        # Try to smooth contours
        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]
        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) 
    ax.imshow(mask_image)

def show_points(coords, labels, ax, marker_size=375):
    pos_points = coords[labels==1]
    neg_points = coords[labels==0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   

def show_box(box, ax):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    

def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):
    for i, (mask, score) in enumerate(zip(masks, scores)):
        plt.figure(figsize=(5, 5))
        plt.imshow(image)
        show_mask(mask, plt.gca(), borders=borders)
        if point_coords is not None:
            assert input_labels is not None
            show_points(point_coords, input_labels, plt.gca())
        if box_coords is not None:
            # boxes
            show_box(box_coords, plt.gca())
        if len(scores) > 1:
            plt.title(f"Mask {i+1}, Score: {score:.3f}", fontsize=18)
        plt.axis('off')
        plt.show()

#Define metric functions
def perimeter_coverage(mask):
    # Get the dimensions of the mask
    height, width = mask.shape
    mask = mask.astype(np.uint8)
    if(np.max(mask)!=255):
        mask = mask*255
    # Extract the perimeter: first and last rows, and first and last columns
    perimeter_pixels = np.concatenate([
        mask[0, :],  # Top row
        mask[-1, :],  # Bottom row
        mask[0:-1, 0],  # Left column
        mask[0:-1, -1]  # Right column
    ])
    if(mask.dtype==np.uint8):
        max_size=2**8 -1
    elif(mask.dtype==np.uint16):
        max_size=2**16 -1
    elif(mask.dtype==np.uint32):
        max_size=2**32 -1
    elif(mask.dtype==np.uint64):
        max_size=2**64 -1
    elif(mask.dtype==np.float16 or mask.dtype==np.float32 or mask.dtype==np.float64):
        max_size=2**32 -1
    else:
        raise TypeError('Mask must be an unsigned interger or float. Was type:'+str(mask.dtype))

    # Total perimeter pixels
    total_perimeter_pixels = len(perimeter_pixels)*max_size

    # Count how many of those pixels are part of the mask (assuming mask is binary 1/0)
    covered_perimeter_pixels = np.sum(perimeter_pixels)

    # Calculate the percentage of the perimeter covered by the mask
    coverage_ratio = covered_perimeter_pixels / total_perimeter_pixels if total_perimeter_pixels != 0 else 0

    return coverage_ratio

#From Claude 3.5 Connet Oct.24, 2024
def unfilled_ratio(img):
    if img.dtype != np.uint8:
        img = img.astype(np.uint8)

    # Apply threshold to get binary image
    _, binary = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY)
    
    # Find contours
    contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return 0
    
    # Find the largest contour (assuming it's the main object)
    main_contour = max(contours, key=cv2.contourArea)
    
    # Calculate total area within the outer contour
    total_area = cv2.contourArea(main_contour)

    # Calculate area of holes
    holes_area = 0
    print(len(contours))
    if hierarchy is not None:
        for i, h in enumerate(hierarchy[0]):
            # If this contour has a parent (meaning it's a hole)
            if h[3] >= 0:  # h[3] is the index of the parent contour
                holes_area += cv2.contourArea(contours[i])
    
    # Calculate ratio
    if total_area == 0:
        return 0
    print('total area: '+str(total_area))
    print('holes area: '+str(holes_area))
    unfilled_ratio = holes_area / total_area

    # # Create visualization
    # # visualization = img.copy()
    # Draw main contour in green
    # cv2.drawContours(visualization, [main_contour], -1, (0, 255, 0), 2)
    # # Draw holes in red
    # for i, h in enumerate(hierarchy[0]):
    #     if h[3] >= 0:  # If it's a hole
    #         cv2.drawContours(visualization, [contours[i]], -1, (0, 0, 255), 2)
    
    # Add text with measurements
    # cv2.putText(visualization, f'Total Area: {total_area:.0f}', (10, 30),
    #             cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    # cv2.putText(visualization, f'Holes Area: {holes_area:.0f}', (10, 60),
    #             cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    # cv2.putText(visualization, f'Ratio: {fill_ratio:.3f}', (10, 90),
    #             cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    # cv2.imshow('fill visualization',visualization)
    # cv2.waitKey(0)
    # cv2.destroyAllWindows()
    return unfilled_ratio

def SAM_surface_segmentation(path):
    all_points = []
    input_label = []
    image = cv2.imread(path)
    image = cv2.resize(image,(input_size,input_size), interpolation = cv2.INTER_AREA)
    if(input_size==1024):
        image = image[0:960,:]
    image_blurred = cv2.blur(cv2.blur(image,(6,6)),(10,10))
    condition_img = image_blurred>253
    non_zero_indices = np.nonzero(np.where(condition_img,1,0))
    #Remove reflective edges
    condition_img = image_blurred>253
    non_zero_indices = np.nonzero(np.where(condition_img,1,0))
    # if len(non_zero_indices[0]) > 0:
    #     for i in range(RANDOM_POSITIONS):
    #         temp_idx = int(random.random()*len(non_zero_indices[0]))
    #         all_points.append([non_zero_indices[1][temp_idx],non_zero_indices[0][temp_idx]])
    #         input_label.append(0)

    for x_pos in [len(image_blurred[0])*.65,len(image_blurred[0])*.5,len(image_blurred[0])*.35]:
        for y_pos in [len(image_blurred[1])*.65,len(image_blurred[0])*.5,len(image_blurred[1])*.35]:
            all_points.append([x_pos,y_pos])
            input_label.append(1)
    image = cv2.resize(image,(input_size,input_size), interpolation = cv2.INTER_AREA)
    predictor.set_image(image)
    masks, scores, logits = predictor.predict(
        point_coords=np.array(all_points),
        point_labels=np.array(input_label),
        multimask_output=False,
    )
    processed_mask = (masks[0]).astype(np.uint8) * 255

    # Step 1: Label connected components
    num_labels, labels_im = cv2.connectedComponents(processed_mask)

    # Step 2: Count pixels for each label
    sizes = np.bincount(labels_im.ravel())

    # Step 3: Find the largest component (ignore the background)
    largest_component_label = sizes[1:].argmax() + 1  # +1 to offset background

    # Step 4: Create a new mask for the largest component
    largest_defect_mask = np.zeros_like(processed_mask)
    largest_defect_mask[labels_im == largest_component_label] = 255
    processed_mask = largest_defect_mask
    # Creating kernel 
    dilate_kernel = np.ones((6, 6), np.uint8)
    erosion_kernal = np.ones((6, 6), np.uint8)
    # Using cv2.erode() method  
    processed_mask = cv2.dilate(processed_mask, dilate_kernel, cv2.BORDER_REFLECT) #
    # processed_mask = cv2.dilate(processed_mask, dilate_kernel, cv2.BORDER_REFLECT) #
    # processed_mask = cv2.dilate(processed_mask, dilate_kernel, cv2.BORDER_REFLECT) #
    processed_mask = cv2.erode(processed_mask, erosion_kernal, cv2.BORDER_REFLECT) 
    processed_mask = cv2.erode(processed_mask, erosion_kernal, cv2.BORDER_REFLECT) 
    processed_mask = cv2.erode(processed_mask, erosion_kernal, cv2.BORDER_REFLECT) 
    processed_mask = cv2.erode(processed_mask, erosion_kernal, cv2.BORDER_REFLECT) 
    processed_mask = cv2.erode(processed_mask, erosion_kernal, cv2.BORDER_REFLECT) 
    processed_mask = cv2.dilate(processed_mask, np.ones((30, 30), np.uint8), cv2.BORDER_REFLECT) #

    # fill_mask_holes from Claude 3.5 Sonnet Oct. 23, 2024
    def fill_mask_holes(mask):
        """
        Fill holes in a binary mask using floodFill.
        
        Parameters:
        mask (numpy.ndarray): Binary input mask (0 and 255 values)
        
        Returns:
        numpy.ndarray: Mask with holes filled
        """
        # Ensure mask is binary and of type uint8
        if mask.dtype != np.uint8:
            mask = mask.astype(np.uint8)
        
        # Threshold to ensure binary image
        _, binary_mask = cv2.threshold(mask, 200, 255, cv2.THRESH_BINARY)
        
        # Create a copy of the mask for flood filling
        # Note: floodFill needs a mask that's 2 pixels bigger in each direction
        h, w = binary_mask.shape
        filled_mask = binary_mask.copy()
        filling_mask = np.zeros((h + 2, w + 2), np.uint8)
        
        # Flood fill from point (0,0)
        cv2.floodFill(filled_mask, filling_mask, (0,0), 255)
        
        # Invert the flood-filled image
        filled_mask_inv = cv2.bitwise_not(filled_mask)
        
        # Combine the original mask with the filled holes
        out_mask = binary_mask | filled_mask_inv
        
        return out_mask
    max_mask = fill_mask_holes(processed_mask)
    return image_blurred, max_mask, all_points, input_label

def setup_SAM():
    sam_checkpoint = path +"/sam_vit_h_4b8939.pth"
    # url = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
    # sam_checkpoint = urllib.request.urlretrieve(url)

    model_type = "vit_h"

    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Only move `sam` to GPU if an NVIDIA GPU is available
    if torch.cuda.is_available():
        sam.to(device=device)
        print("Model moved to GPU.")
    else:
        print("No NVIDIA GPU detected. Using CPU.")
    predictor = SamPredictor(sam)

    np.random.seed(3)
    return predictor
# %%
if __name__=="main":
    predictor = setup_SAM()
    idx += 1
    d_pos = .3
    grid_spacing = 32
    RANDOM_POSITIONS=20
    input_size = 1024
    combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
    df = combined_df[combined_df['image_basename'].apply(lambda x: type(x)==str)] #Drops nas, which are loaded as floats
    df = df.groupby('sample_id')

    stitched_imgs = pd.DataFrame()
    for group_string, sample in df:
        if 'stitched' in sample['image_class'].value_counts().index:
            stitched_imgs = pd.concat(
                [
                    stitched_imgs,
                    sample[sample['image_class']=='stitched'].iloc[[0]]
                ],
                axis=0
            )

    organize_data.print_column_counts(stitched_imgs)
    print(stitched_imgs['image_path'].iloc[0])
    input_image, output_image, all_points, input_label = SAM_surface_segmentation(stitched_imgs['image_path'].iloc[idx])
    fig, ax = plt.subplots(2)
    ax[0].imshow(input_image)
    show_points(np.array(all_points), np.array(input_label), ax[0])
    ax[1].imshow(output_image)
    plt.show()
else:
    print(__name__+" functions loaded")

```



## initiating_defect_features.py



```{python, eval = FALSE}
# %%
'''Setup'''
import cv2
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import ast
from joblib import Parallel, delayed
import seaborn as sns
# import scipy
import sklearn.linear_model
import sklearn.metrics
import sklearn.model_selection
import sklearn.preprocessing
import sklearn.pipeline
import itertools
sns.set(font_scale=1.5)
# %%
'''Functions'''
def process_mask(mask):
    # Find connected components
    num_labels, labels = cv2.connectedComponents(mask)
    
    # Count components (excluding background)
    num_objects = num_labels - 1
    
    # Find largest object
    largest_object_mask = np.zeros_like(mask)
    if num_objects > 0:
        # Get unique labels, excluding background (0)
        label_counts = [np.sum(labels == i) for i in range(1, num_labels)]
        largest_object_label = np.argmax(label_counts) + 1
        largest_object_mask = (labels == largest_object_label).astype(np.uint8) * 255
    
    return num_objects, largest_object_mask


def extract_largest_object(img,points_list):
    mask = np.zeros(img.shape[:2], dtype=np.uint8)
    cv2.fillPoly(
        mask,
        [np.array(points_list, dtype=np.int32)],
        255
    )
    img_object, largest_object_mask = process_mask(mask)
    return largest_object_mask

def find_sharpness(numpy_array):
    #Convert array to dataframe
    y,x= np.nonzero(numpy_array)
    df = pd.DataFrame({'x':x,'y':y})
    x0 = df['x'].sum()/df['x'].count()
    y0 = df['y'].sum()/df['y'].count()
    df = df[(df['x']!=0)&(df['y']!=0)].reset_index(drop=True) # to avoid divide by 0 errors later
    #Calculate polar coordinates
    df['x_rel'] = df['x'] - x0
    df['y_rel'] = df['y'] - y0
    df['angle'] = df.apply(lambda row:math.atan2(row['y_rel'],row['x_rel']),axis=1)
    df['distance'] = df.apply(lambda row:math.sqrt(row['y_rel']**2 + row['x_rel']**2),axis=1)
    global_max = df['distance'].max()
    #Find max for each bin
    num_bins = 180
    bin_edges = np.linspace(-math.pi/2, math.pi/2, num_bins + 1)
    bins = pd.IntervalIndex.from_breaks(bin_edges,name='Angle_bin')
    df.index = pd.cut(df['angle'],bins)
    max_df = df.groupby(level=0,observed=False)['distance'].max()
    max_diff = []
    for i in range(0,len(max_df)-1):
        max_diff.append(abs(max_df.iloc[i]-max_df.iloc[i+1])/global_max)
    return max(max_diff) if max_diff else 0

def calculate_aspect_ratio(mask):
    # Find the non-zero mask coordinates
    y_coords, x_coords = np.where(mask > 0)
    
    # Calculate the bounding box dimensions
    height = y_coords.max() - y_coords.min() + 1
    width = x_coords.max() - x_coords.min() + 1
    
    # Calculate the aspect ratio
    aspect_ratio = width / height
    return aspect_ratio

def get_perimeter(binary_image):
    # Ensure the image is binary
    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # If there are contours, return the perimeter of the largest contour
    if contours:
        largest_contour = max(contours, key=cv2.contourArea)
        return cv2.arcLength(largest_contour, closed=True)
    
    return 0
    
def calculate_aspect_ratio_rotated(binary_image):
    # Find contours
    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if not contours:
        return None
    
    # Get the largest contour
    largest_contour = max(contours, key=cv2.contourArea)
    
    # Get the rotated rectangle
    rect = cv2.minAreaRect(largest_contour)
    (width, height) = rect[1]
    
    # Calculate aspect ratio (ensuring width is always the larger dimension)
    aspect_ratio = max(width, height) / min(width, height)
    
    return aspect_ratio

def multiple_linear_regression(X,Y):
    # Sklearn Linear Regression
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)
    
    # Scale features
    scaler = sklearn.preprocessing.StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Fit model
    lr = sklearn.linear_model.LinearRegression()
    lr.fit(X_train_scaled, y_train)
    
    # Predict and evaluate
    y_pred = lr.predict(X_test_scaled)
    mse = sklearn.metrics.mean_squared_error(y_test, y_pred)
    r2 = sklearn.metrics.r2_score(y_test, y_pred)
    return lr, y_pred, mse, r2

def polynomial_regression(X,Y,degree=3):
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)
    
    # Create polynomial pipeline
    poly_model = sklearn.pipeline.make_pipeline(
        sklearn.preprocessing.StandardScaler(),
        sklearn.preprocessing.PolynomialFeatures(degree),
        sklearn.linear_model.LinearRegression()
    )

    # Fit model
    poly_model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = poly_model.predict(X_test)
    mse = sklearn.metrics.mean_squared_error(y_test, y_pred)
    r2 = sklearn.metrics.r2_score(y_test, y_pred)
    return poly_model, y_pred, mse, r2

# 3. Ridge Regression
def ridge_regression(X,Y):
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)
    
    # Scale features
    scaler = sklearn.preprocessing.StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
        
    # Ridge Regression
    ridge = sklearn.linear_model.Ridge(alpha=1.0)
    ridge.fit(X_train_scaled, y_train)

    # Predict and evaluate
    y_pred = ridge.predict(X_test_scaled)
    mse = sklearn.metrics.mean_squared_error(y_test, y_pred)
    r2 = sklearn.metrics.r2_score(y_test, y_pred)
    return ridge, y_pred, mse, r2

# 4. Lasso Regression
def lasso_regression(X,Y):
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)
    
    # Scale features
    scaler = sklearn.preprocessing.StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
        
    # Lasso Regression
    lasso = sklearn.linear_model.Lasso(alpha=0.1)
    lasso.fit(X_train_scaled, y_train)

    # Predict and evaluate
    y_pred = lasso.predict(X_test_scaled)
    mse = sklearn.metrics.mean_squared_error(y_test, y_pred)
    r2 = sklearn.metrics.r2_score(y_test, y_pred)
    return lasso, y_pred, mse, r2

def make_feature_df(points_df):
    imgs = []
    energy = []
    stress = []
    laser_power = []
    laser_speed = []
    cycles = []
    sample_id = []
    for row in points_df.iterrows():
        imgs.append(
            extract_largest_object(
                cv2.imread(row[1]['image_path']),
                ast.literal_eval(row[1]['points'])
                )
        )
        laser_power.append(row[1]['scan_power_W'])
        laser_speed.append(row[1]['scan_velocity_mm_s'])
        energy.append(row[1]['energy_density_J_mm3'])
        stress.append(row[1]['test_stress_Mpa'])
        cycles.append(row[1]['cycles'])
        sample_id.append(row[1]['sample_id'])
    portion_of_screen = Parallel(n_jobs=-1)(delayed(lambda x: x.sum() / (x.size * x.max()))(x) for x in imgs)
    max_sharpness = Parallel(n_jobs=-1)(delayed(find_sharpness)(x) for x in imgs)
    aspect_ratio = Parallel(n_jobs=-1)(delayed(calculate_aspect_ratio_rotated)(x) for x in imgs)
    perimeter = Parallel(n_jobs=-1)(delayed(get_perimeter)(x) for x in imgs)
    pixels = Parallel(n_jobs=-1)(delayed(lambda x: x.sum() / x.max())(x) for x in imgs)
    pixel_perimeter_ratio = Parallel(n_jobs=-1)(delayed(lambda x: x.sum() / (x.max() * get_perimeter(x)))(x) for x in imgs)
    data = {
        "sample_id":sample_id,
        "imgs": imgs,
        "screen_portion": portion_of_screen,
        "max_sharpness": max_sharpness,
        "aspect_ratio": aspect_ratio,
        "perimeter": perimeter,
        "pixels": pixels,
        "pixel_perimeter_ratio": pixel_perimeter_ratio,
        "energy_density":energy,
        "stress_Mpa":stress,
        "laser_power":laser_power,
        "laser_speed":laser_speed,
        "cycles": cycles
    }
    return pd.DataFrame(data)

def plot_feature_df(df):
    def annotate_r2(x, y, **kwargs):
        x = np.array(x).reshape(-1, 1)
        y = np.array(y)
        model = sklearn.linear_model.LinearRegression()
        model.fit(x, y)
        r_squared = model.score(x, y)
        ax = plt.gca()
        ax.annotate(f"$R^2$ = {r_squared:.2f}", xy=(0.05, 0.9), xycoords=ax.transAxes, fontsize=10)

    # Create pairplot with linear regression
    g = sns.pairplot(df, kind="reg", plot_kws={"line_kws": {"color": "red"}})
    # Add R-squared annotations to each plot
    g.map(annotate_r2)
# def regression_on_df(df,regression_function_list=[multiple_linear_regression,polynomial_regression,ridge_regression,lasso_regression]):
#     metric_used_list = []
#     regression_type = []
#     energy_density_list = []
#     laser_power_list = []
#     scan_speed_list = []
#     aspect_ratio_list = []
#     sharpness_list = []
#     y_pred_list = []
#     mse_list = []
#     r2_list = []
#     for metric in ['log_stress','stress']:
#         for regression in regression_function_list:
#             for inputs in list(itertools.product([True, False], repeat=5)):
#                 if True in inputs: # There needs to be at least 1 predictor
#                     regression_type.append(regression.__name__)
#                     inputs_columns = []
#                     if inputs[0]:
#                         energy_density_list.append(True)
#                         inputs_columns.append('energy_density')
#                     else:
#                         energy_density_list.append(False)
#                     if inputs[1]:
#                         laser_power_list.append(True)
#                         inputs_columns.append('laser_power')
#                     else:
#                         laser_power_list.append(False)
#                     if inputs[2]:
#                         scan_speed_list.append(True)
#                         inputs_columns.append('laser_speed')
#                     else:
#                         scan_speed_list.append(False)
#                     if inputs[3]:
#                         aspect_ratio_list.append(True)
#                         inputs_columns.append('aspect_ratio')
#                     else:
#                         aspect_ratio_list.append(False)
#                     if inputs[4]:
#                         sharpness_list.append(True)
#                         inputs_columns.append('max_sharpness')
#                     else:
#                         sharpness_list.append(False)
#                     if metric =='log_stress':
#                         Y = df['cycles'].apply(math.log)
#                         metric_used_list.append('log(stress)')
#                     elif metric =='stress':
#                         Y = df['cycles'].apply(math.log)*df['stress_Mpa']
#                         metric_used_list.append('stress')
#                     X = df[inputs_columns]
#                     model, y_pred, mse, r2 = regression(X,Y)
#                     mse_list.append(mse)
#                     r2_list.append(r2)

#     regression_dict = {
#         "metric_used":metric_used_list,
#         "regression_type":regression_type,
#         "energy_density":energy_density_list,
#         "laser_power":laser_power_list,
#         "scan_speed":scan_speed_list,
#         "aspect_ratio":aspect_ratio_list,
#         "sharpness":sharpness_list,
#         "mse":mse_list,
#         "r2":r2_list
#     }
#     df_results = pd.DataFrame(regression_dict)
#     return df_results

def regression_on_df(
    df, 
    regression_function_list=[multiple_linear_regression, polynomial_regression, ridge_regression, lasso_regression],
    features = ['energy_density', 'laser_power', 'laser_speed', 'aspect_ratio', 'max_sharpness'],
    metrics = ['log_stress', 'stress']):
    results = []

    for metric in metrics:
        for regression in regression_function_list:
            # Generate all True/False combinations for each feature
            for inputs in itertools.product([True, False], repeat=len(features)):
                if any(inputs):  # At least one predictor must be True
                    # Determine which features are selected
                    selected_features = [f for f, include in zip(features, inputs) if include]

                    # Prepare Y based on the metric
                    if metric == 'log_stress':
                        Y = df['cycles'].apply(math.log)
                        metric_used = 'log(stress)'
                    else:  # metric == 'stress'
                        Y = df['cycles'].apply(math.log)*df['stress_Mpa']
                        metric_used = 'stress'

                    # Prepare X from selected features
                    X = df[selected_features]

                    # Run the regression
                    model, y_pred, mse, r2 = regression(X, Y)

                    # Build the result dictionary
                    result = {
                        "metric_used": metric_used,
                        "regression_type": regression.__name__,
                        "mse": mse,
                        "r2": r2
                    }
                    
                    # Add information about which features were used
                    for f, include in zip(features, inputs):
                        result[f] = include

                    results.append(result)

    # Convert the results into a DataFrame
    df_results = pd.DataFrame(results)
    return df_results

# %%
if __name__=="__main__":
    print(__name__+" script running")
    # %%
    combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
    points_df = combined_df[~combined_df['points'].isna()]
    print(points_df['image_class'].value_counts())
    print(points_df['sample_id'].value_counts().head(10))
    df = make_feature_df(points_df)
    columns = ["screen_portion","max_sharpness","aspect_ratio","perimeter","pixels"]
    df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)
    df = df.dropna()
    results_df = regression_on_df(df)
    print(results_df.loc[results_df["r2"].idxmax()])
    print(results_df.loc[results_df[(results_df['aspect_ratio']==False) &(results_df['sharpness']==False)]["r2"].idxmax()])    
    plot = plot_feature_df(df[columns])
else:
    print(__name__+" functions loaded")

```



## initiating_defect_mask_validation.py



```{python, eval = FALSE}
# %%
'''Set Up'''
import math
import sys
import numpy as np
import pandas as pd
import cv2
import torch
import matplotlib.pyplot as plt
import sklearn
import initiating_defect_features
from segment_anything import sam_model_registry, SamPredictor
sys.path.append("/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/fractography_scripts/pykan/")
from pykan.kan import *

energy_density_label = "Energy Density [J/mm^3]"

def KAN_regression(X,Y):
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)
    model = KAN(width=[X_test.shape[1],X_test.shape[1],1],k=3,device=device)
    X_train = torch.Tensor(X_train.values).to(device)
    X_test = torch.Tensor(X_test.values).to(device)
    y_train = torch.Tensor(y_train.values).to(device)
    y_test = torch.Tensor(y_test.values).to(device)
    data = {
        'train_input':X_train,
        'test_input':X_test,
        'train_label':y_train,
        'test_label':y_test
    }
    model.fit(dataset=data, opt="LBFGS", steps=100, lamb=0.01, reg_metric='edge_forward_spline_n')
    def r2_score(y_true, y_pred):
        # Ensure that tensors are 1-dimensional if necessary
        y_true = y_true.view(-1)
        y_pred = y_pred.view(-1)
        
        # Compute the residual sum of squares
        ss_res = torch.sum((y_true - y_pred) ** 2)
        
        # Compute the total sum of squares (proportional to variance of the data)
        ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)
        mse = ss_res / y_true.size(0)

        # Compute R²
        # Add a small epsilon in the denominator if there's a chance of division by zero.
        r2 = 1 - (ss_res / (ss_tot + 1e-12))
        
        return r2, mse
    y_pred = model(data['test_input'])
    r2, mse = r2_score(data['test_label'],y_pred)
    return model, y_pred.item(), r2.item(), mse.item()

def find_centroid(numpy_array):
    y,x= np.nonzero(numpy_array)
    df = pd.DataFrame({'x':x,'y':y})
    x0 = df['x'].sum()/df['x'].count()
    y0 = df['y'].sum()/df['y'].count()
    return [x0,y0]

def format_to_SAM(img):
    if(type(img)!=np.ndarray):
        raise TypeError(f"Input to form_to_SAM function must be numpy array, was{type(img)}")
    img = cv2.resize(img,(1024,1024), interpolation = cv2.INTER_AREA)
    if img.ndim == 2:  # Grayscale image
        img = np.repeat(img[:, :, np.newaxis], 3, axis=2)
    img = np.array(img,dtype=np.uint8)
    return img

def process_row(img, xy, model):
    try:
        img = format_to_SAM(img)
        model.set_image(img)
        output = model.predict(
            point_coords = np.expand_dims(xy,axis=0),
            point_labels=np.array([1])
        )
        return np.array(output[0]*255,dtype=np.uint8)
    except Exception as e:
        print(row.index)
        print(e)
        return np.nan
def process_mask(mask):
    # Find connected components
    try:
        # print(type(mask))
        # mask = np.array(mask, dtype=np.uint8)
        # print(mask.sum())
        # print(mask.shape)
        mask = np.transpose(mask, (1, 2, 0))
        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
        num_labels, labels = cv2.connectedComponents(mask)
        
        # Count components (excluding background)
        num_objects = num_labels - 1
        
        # Find largest object
        largest_object_mask = np.zeros_like(mask)
        if num_objects > 0:
            # Get unique labels, excluding background (0)
            label_counts = [np.sum(labels == i) for i in range(1, num_labels)]
            largest_object_label = np.argmax(label_counts) + 1
            largest_object_mask = (labels == largest_object_label).astype(np.uint8) * 255
        
        return largest_object_mask
    except Exception as e:
        print(e)
        return np.NAN
def invert_mask(mask):
    """
    Invert a binary mask (0 and 255) by swapping background and foreground.
    Parameters:
        mask (np.ndarray): 2D numpy array representing a binary mask, 
                        expected to have values either 0 or 255.

    Returns:
        np.ndarray: Inverted mask.
    """
    if not isinstance(mask, np.ndarray):
        raise TypeError("Input must be a numpy array.")

    if mask.ndim != 2:
        raise ValueError("Input mask must be a 2D array.")

    # Ensure mask is uint8
    mask = mask.astype(np.uint8, copy=False)

    # Invert the mask
    inverted = 255 - mask
    return inverted
# %%
if __name__=="__main__":
    print(__name__+" script being executed")
    # %%
    '''Load df and SAM'''
    combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
    points_df = combined_df[~combined_df['points'].isna()]
    df = initiating_defect_features.make_feature_df(points_df)
    columns = ["screen_portion","max_sharpness","aspect_ratio","perimeter","pixels"]
    df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)
    df = df.dropna()
    # path = "/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/sam"
    # # sam_checkpoint = path +"/sam_vit_h_4b8939.pth"
    # url = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
    # sam_checkpoint = urllib.request.urlretrieve(url)

    model_type = "vit_h"

    sam = sam_model_registry[model_type](checkpoint="sam_vit_h_4b8939.pth")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Only move `sam` to GPU if an NVIDIA GPU is available
    if torch.cuda.is_available():
        sam.to(device=device)
        print("Model moved to GPU.")
    else:
        print("No NVIDIA GPU detected. Using CPU.")
    SAM = SamPredictor(sam)

    np.random.seed(3)
    
    df['xy'] = df['imgs'].apply(find_centroid).apply(np.array)
    best_rows = []


    # %%
    '''Find Cross Entropy'''
    df['SAM_raw_output'] =df.apply(lambda x: process_row(x['imgs'],x['xy'],SAM),axis=1)
    df['SAM_processed_output'] = df['SAM_raw_output'].apply(process_mask).apply(invert_mask)
    df["cross_entropy"] = df.apply(
        lambda x: torch.nn.functional.binary_cross_entropy(
            torch.Tensor(cv2.resize(x['imgs'],(1024,1024))/255),
            torch.Tensor(x['SAM_processed_output']/255)
        )
    ,axis=1).apply(lambda x: x.detach().item())
    # %%
    '''Find Best Rows'''
    for group_string, group in df.groupby(by="sample_id"):
        best_rows.append(
            group.loc[group['cross_entropy'].idxmin()]
        )
    best_rows_df = pd.DataFrame(best_rows)
    # best_rows_df.to_csv("best_rows.csv")
    # #  %%
    # '''Show Good Example'''
    # good_ex_fig, good_ex_ax = plt.subplots(1, 2, tight_layout=True)
    # min_idx = df['cross_entropy'].idxmin()
    # good_ex_ax[0].imshow(cv2.resize(df['imgs'].iloc[min_idx],(1024,1024)))
    # good_ex_ax[1].imshow(df['SAM_processed_output'].iloc[min_idx])
    # good_ex_fig.show()
    # print(df['cross_entropy'].iloc[min_idx])
    # print(f"Input shape: {df['imgs'].iloc[min_idx].shape}")
    # print(f"Output shape: {df['SAM_processed_output'].iloc[min_idx].shape}")
    # #  %%
    # '''Show Bad Example'''
    # bad_ex_fig, bad_ex_ax = plt.subplots(1, 2, tight_layout=True)
    # max_idx = df['cross_entropy'].idxmax()
    # bad_ex_ax[0].imshow(cv2.resize(df['imgs'].iloc[max_idx],(1024,1024)))
    # bad_ex_ax[1].imshow(df['SAM_processed_output'].iloc[max_idx])
    # bad_ex_fig.show()
    # print(df['cross_entropy'].iloc[max_idx])
    # print(f"Input shape: {df['imgs'].iloc[max_idx].shape}")
    # print(f"Output shape: {df['SAM_processed_output'].iloc[max_idx].shape}")

    # %%
    '''Hist Entropy'''    
    hist_fig, hist_ax = plt.subplots(1, 1, tight_layout=True)
    hist_ax.hist(df['cross_entropy'],bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])
    hist_fig.show()
    # %%
    '''screen portion vs cross entropy'''
    # Can't be too big
    pve_fig, (pve_ax,good_pve_ax,bad_pve_ax) = plt.subplots(3, 1, tight_layout=True,figsize=(16,20))
    pve_scatter = pve_ax.scatter(df['screen_portion'],df['cross_entropy'],
        c=df['energy_density'],
        cmap='inferno',  # or another colormap you prefer
        vmin=df['energy_density'].min(),
        vmax=df['energy_density'].max()/2)
    pve_ax.set_xlabel("Portion of Screen")
    pve_ax.set_ylabel("Binary Cross Entropy")
    print(df['cross_entropy'].apply(lambda x:x<2).value_counts().rename("Enrtopy<2"))
    good_df = df[df['cross_entropy'].apply(lambda x:x<2)].copy()
    good_pve_scatter = good_pve_ax.scatter(good_df['screen_portion'],good_df['cross_entropy'],
        c=good_df['energy_density'],
        cmap='inferno',  # or another colormap you prefer
        vmin=df['energy_density'].min(),
        vmax=df['energy_density'].max()/2)
    good_pve_ax.set_xlabel("Portion of Screen")
    good_pve_ax.set_ylabel("Binary Cross Entropy")
    # Compute a linear fit using numpy's polyfit
    good_x = good_df['screen_portion'].values
    good_y = good_df['cross_entropy'].values
    good_m, good_b = np.polyfit(good_x, good_y, 1)
    good_y_pred = good_m * good_x + good_b
    good_y_mean = np.mean(good_y)
    good_ss_tot = np.sum((good_y - good_y_mean)**2)
    good_ss_res = np.sum((good_y - good_y_pred)**2)
    good_r_squared = 1 - (good_ss_res / good_ss_tot)
    # Plot the linear regression line
    good_pve_ax.plot(good_x, good_y_pred, color='red', label=f'{round(good_m,1)}*x+{round(good_b,1)}, R^2={round(good_r_squared,3)}')
    bad_df = df[df['cross_entropy'].apply(lambda x:x>2)].copy()
    bad_pve_scatter = bad_pve_ax.scatter(bad_df['screen_portion'],bad_df['cross_entropy'],
        c=bad_df['energy_density'],
        cmap='inferno',  # or another colormap you prefer
        vmin=df['energy_density'].min(),
        vmax=df['energy_density'].max()/2)
    bad_pve_ax.set_xlabel("Portion of Screen")
    bad_pve_ax.set_ylabel("Binary Cross Entropy")
    # Compute a linear fit using numpy's polyfit
    bad_x = bad_df['screen_portion'].values
    bad_y = bad_df['cross_entropy'].values
    bad_m, bad_b = np.polyfit(bad_x, bad_y, 1)
    bad_y_pred = bad_m * bad_x + bad_b
    bad_y_mean = np.mean(bad_y)
    bad_ss_tot = np.sum((bad_y - bad_y_mean)**2)
    bad_ss_res = np.sum((bad_y - bad_y_pred)**2)
    bad_r_squared = 1 - (bad_ss_res / bad_ss_tot)
    bad_m, bad_b = np.polyfit(bad_df['screen_portion'], bad_df['cross_entropy'], 1)
    bad_pve_ax.plot(bad_x, bad_y_pred, color='red', label=f'{round(bad_m,1)}*x+{round(bad_b,1)},R^2={round(bad_r_squared,3)}')
    bad_pve_ax.legend()
    pve_fig.colorbar(bad_pve_scatter, ax=bad_pve_ax, label=energy_density_label)
    good_pve_ax.legend()
    pve_fig.colorbar(good_pve_scatter, ax=good_pve_ax, label=energy_density_label)
    good_pve_ax.legend()
    pve_ax.plot(good_x, good_y_pred, color='red', label=f'{round(good_m,1)}*x+{round(good_b,1)},R^2={round(good_r_squared,3)}')
    pve_ax.plot(bad_x, bad_y_pred, color='red', label=f'{round(bad_m,1)}*x+{round(bad_b,1)},R^2={round(bad_r_squared,3)}')
    pve_ax.legend()
    pve_fig.colorbar(pve_scatter, ax=pve_ax, label=energy_density_label)
    pve_fig.show()
    # %%
    '''entropy vs energy density'''
    # Not as effective outside of process region
    entropy_vs_energy_fig, entropy_vs_energy_ax = plt.subplots(1, 1, tight_layout=True)
    entropy_vs_energy_scatter = entropy_vs_energy_ax.scatter(good_df['energy_density'],good_df['cross_entropy'],
        c=good_df['energy_density'],
        cmap='inferno',  # or another colormap you prefer
        vmin=good_df['energy_density'].min(),
        vmax=good_df['energy_density'].max())
    entropy_vs_energy_ax.set_xlabel("Energy Density [J/mm^3]")
    entropy_vs_energy_ax.set_ylabel("Binary Cross Entropy")
    entropy_vs_energy_fig.colorbar(entropy_vs_energy_scatter, ax=entropy_vs_energy_ax, label=energy_density_label)
    entropy_vs_energy_fig.show()
    # %%
    '''Stress vs entropy'''
    # Roughly normally distributed vs stress
    stress_vs_entropy_fig, stress_vs_entropy_ax = plt.subplots(1, 1, tight_layout=True)
    stress_vs_entropy_scatter = stress_vs_entropy_ax.scatter(
        x=good_df['stress_Mpa'],
        y=good_df['cross_entropy'],
        c=good_df['energy_density'],
        cmap='inferno',  # or another colormap you prefer
        vmin=good_df['energy_density'].min(),
        vmax=good_df['energy_density'].max()/2
    )
    stress_vs_entropy_ax.set_xlabel("Stress[MPa]")
    stress_vs_entropy_ax.set_ylabel("Binary Cross Entropy")
    stress_vs_entropy_fig.colorbar(stress_vs_entropy_scatter, ax=stress_vs_entropy_ax, label=energy_density_label)
    stress_vs_entropy_fig.show()
    # %%
    '''Regression All Images'''
    features = ['energy_density', 'laser_power', 'laser_speed', 'aspect_ratio', 'max_sharpness','pixel_perimeter_ratio']
    columns = ["screen_portion","max_sharpness","aspect_ratio","perimeter","pixels",'cross_entropy']
    regressions = [initiating_defect_features.multiple_linear_regression,
        initiating_defect_features.polynomial_regression,
        initiating_defect_features.ridge_regression,
        initiating_defect_features.lasso_regression,
        ]
    
    df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)
    df = df.dropna()
    all_results_df = initiating_defect_features.regression_on_df(df,
        regression_function_list=regressions,
        features=features)
    # initiating_defect_features.plot_feature_df(df)
    print(all_results_df.loc[all_results_df["r2"].idxmax()])
    print(all_results_df.loc[all_results_df[(all_results_df['aspect_ratio']==False) &(all_results_df['max_sharpness']==False)]["r2"].idxmax()])    

    # %%
    '''Regression Good Dataframe'''
    good_df[columns] = good_df[columns].replace([np.inf, -np.inf], np.nan)
    good_df = good_df.dropna()
    good_results_df = initiating_defect_features.regression_on_df(good_df,
        regression_function_list=regressions,
        features=features)
    # initiating_defect_features.plot_feature_df(good_df)
    print(good_results_df.loc[good_results_df["r2"].idxmax()])
    print(good_results_df.loc[good_results_df[(good_results_df['aspect_ratio']==False) &(good_results_df['max_sharpness']==False)]["r2"].idxmax()])    

    # %%
    '''Regression best rows'''
    good_best_df = best_rows_df[best_rows_df['cross_entropy'].apply(lambda x:x<2)]
    good_best_df[features] = good_best_df[features].replace([np.inf, -np.inf], np.nan)
    good_best_df = good_best_df.dropna()
    good_best_df_results = initiating_defect_features.regression_on_df(
        good_best_df,
        regression_function_list=regressions,
        features=features)
    # initiating_defect_features.plot_feature_df(good_df)
    print(good_best_df_results.loc[good_best_df_results["r2"].idxmax()])
    print(good_best_df_results.loc[good_best_df_results[(good_best_df_results['aspect_ratio']==False) &(good_best_df_results['max_sharpness']==False)&(good_best_df_results['pixel_perimeter_ratio']==False)]["r2"].idxmax()])    

# %%
else:
    print(__name__+" functions loaded")

```
