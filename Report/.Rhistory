x = np.linspace(20, 120, 100)
SN_ax.plot(x, slope * x + intercept, label=f'Linear fit: y = {slope:.2f}x + {intercept:.2f}', color='red')
SN_ax.set_ylabel('log(Cycles)*stress [Mpa]')
SN_ax.set_xlabel('Energy Density [J/mm^3]')
SN_ax.text(SN_ax.get_xlim()[1],SN_ax.get_ylim()[1],f"R^2={r_value:.2f}", fontsize = 24,ha='right', va='top')
SN_fig.tight_layout(h_pad=3)
SN_fig.show()
cmap = plt.get_cmap('inferno')
fig, (ax_scatter,ax_hist_x) = plt.subplots(2,1,figsize=(14,10),height_ratios=[3,1])
groups = combined_df.sort_values(by='cycles').groupby(['energy_density_J_mm3'])
for i, (group, group_data) in enumerate(groups):
Cycles = list(map(math.log10,group_data['cycles']))
Stress = list(map(math.log10,group_data['test_stress_Mpa']))
ax_scatter.scatter(
Cycles,
Stress,
color=cmap(group_data['energy_density_J_mm3']/100))
ax_scatter.set_ylabel('log(Stress [Mpa])')
# ax_scatter.set_xlabel('log(Cycles)')
ax_scatter.set_title('Log-Log Scatter Plot of SN Data')
# colorbar = fig.colorbar(ax.collections[0], ax=ax)
ax_hist_x.hist(list(map(math.log10, combined_df['cycles'])), bins=30, orientation='vertical')
ax_hist_x.set_ylabel('Frequency')
ax_hist_x.set_xlabel("log(Cycles)")
plt.subplots_adjust(hspace=0)
plt.show()
x_SAM_process = []
y_SAM_process = []
x_SAM_test = []
y_SAM_test = []
x_not_SAM_process = []
y_not_SAM_process = []
x_not_SAM_test = []
y_not_SAM_test = []
for group_string, group in combined_df.groupby('sample_id'):
no_points = group[(group['image_class']=='initiation') & (group['points'].isna())]
points = group[(group['image_class']=='initiation') & (~group['points'].isna())]
if len(no_points.index)>=1:
x_not_SAM_process.append(no_points['scan_velocity_mm_s'].iloc[0])
y_not_SAM_process.append(no_points['scan_power_W'].iloc[0])
x_not_SAM_test.append(no_points['cycles'].iloc[0])
y_not_SAM_test.append(no_points['test_stress_Mpa'].iloc[0])
elif  len(points.index)>=1:
x_SAM_process.append(points['scan_velocity_mm_s'].iloc[0])
y_SAM_process.append(points['scan_power_W'].iloc[0])
x_SAM_test.append(points['cycles'].iloc[0])
y_SAM_test.append(points['test_stress_Mpa'].iloc[0])
def jitter(arr, jitter_amount=2):
return arr + np.random.uniform(-jitter_amount, jitter_amount, len(arr))
success_scatter_fig,(SN_axs,PV_axs) = plt.subplots(2,1,figsize=(14,10))
plt.figure(figsize=(14, 10))
# Process Variables Plot
SN_axs.scatter(jitter(x_not_SAM_process), jitter(y_not_SAM_process), color='red', label='No Points', alpha=0.7)
SN_axs.scatter(jitter(x_SAM_process), jitter(y_SAM_process), color='blue', label='With Points', alpha=0.7)
SN_axs.set_xlabel('Scan Velocity (mm/s)',fontsize=16)
SN_axs.set_ylabel('Scan Power (W)',fontsize=16)
SN_axs.set_title('Process Variables',fontsize=24)
SN_axs.legend(fontsize=24)
# Test Variables Plot
PV_axs.scatter(jitter(x_not_SAM_test), jitter(y_not_SAM_test), color='red', label='No Points', alpha=0.7)
PV_axs.scatter(jitter(x_SAM_test), jitter(y_SAM_test), color='blue', label='With Points', alpha=0.7)
PV_axs.set_xlabel('Cycles',fontsize=16)
PV_axs.set_ylabel('Test Stress (MPa)',fontsize=16)
PV_axs.set_title('Test Variables',fontsize=24)
PV_axs.legend(fontsize=16)
success_scatter_fig.tight_layout()
success_scatter_fig.show()
SAM_process = []
SAM_test = []
not_SAM_process = []
not_SAM_test = []
for group_string, group in combined_df.groupby('sample_id'):
no_points = group[(group['image_class']=='initiation') & (group['points'].isna())]
points = group[(group['image_class']=='initiation') & (~group['points'].isna())]
if not (len(no_points.index)>=1 and len(points.index)>=1):
if len(no_points.index)>=1:
not_SAM_process.append(no_points['energy_density_J_mm3'].iloc[0])
elif  len(points.index)>=1:
SAM_process.append(points['energy_density_J_mm3'].iloc[0])
plt.rcParams.update({'font.size': 32})  # Set font size for all elements
plt.figure(figsize=(10, 5))
# Process Variables Histograms
plt.hist(x_not_SAM_process, bins='auto', color='red', alpha=0.7, label='No Points')
plt.hist(x_SAM_process, bins='auto', color='blue', alpha=0.7, label='With Points')
plt.xlabel('Energy Density [J/mm^3]',fontsize=16)
plt.ylabel('Frequency',fontsize=16)
plt.title('Process Variables',fontsize=24)
plt.legend(fontsize=16)
plt.show()
x_not_SAM_process = np.array(x_not_SAM_process)
x_SAM_process = np.array(x_SAM_process)
x_not_SAM_process = x_not_SAM_process[~np.isnan(x_not_SAM_process)]
x_SAM_process = x_SAM_process[~np.isnan(x_SAM_process)]
statistic, p_value = scipy.stats.ks_2samp(np.array(x_not_SAM_process),np.array(x_SAM_process))
print("P value: "+str(p_value))
if p_value < 0.05:
print("Reject the null hypothesis: distributions are different")
else:
print("Fail to reject the null hypothesis: distributions are the same")
x_SAM_test = []
x_not_SAM_test = []
for group_string, group in combined_df.groupby('sample_id'):
no_points = group[(group['image_class']=='initiation') & (group['points'].isna())]
points = group[(group['image_class']=='initiation') & (~group['points'].isna())]
if not (len(no_points.index)>=1 and len(points.index)>=1):
if len(no_points.index)>=1:
cycles =no_points['cycles'].iloc[0]
stress =no_points['test_stress_Mpa'].iloc[0]
x_not_SAM_test.append(math.log(cycles)*math.log(stress))
elif  len(points.index)>=1:
cycles =points['cycles'].iloc[0]
stress =points['test_stress_Mpa'].iloc[0]
x_SAM_test.append(math.log(cycles)*math.log(stress))
plt.rcParams.update({'font.size': 32})  # Set font size for all elements
plt.figure(figsize=(10, 5))
# Process Variables Histograms
plt.subplot(1, 2, 1)
plt.hist(x_not_SAM_test, bins='auto', color='red', alpha=0.7, label='No Points')
plt.hist(x_SAM_test, bins='auto', color='blue', alpha=0.7, label='With Points')
plt.xlabel('log(cycles)*los(stress [Mpa])',fontsize=16)
plt.ylabel('Frequency',fontsize=16)
plt.title('Testing Variables',fontsize=20)
plt.legend(fontsize=16)
plt.show()
x_not_SAM_test = np.array(x_not_SAM_test)
x_SAM_test = np.array(x_SAM_test)
x_not_SAM_test = x_not_SAM_test[~np.isnan(x_not_SAM_test)]
x_SAM_test = x_SAM_test[~np.isnan(x_SAM_test)]
statistic, p_value = scipy.stats.ks_2samp(np.array(x_not_SAM_test),np.array(x_SAM_test))
print("P value: "+str(p_value))
if p_value < 0.05:
print("Reject the null hypothesis: distributions are different")
else:
print("Fail to reject the null hypothesis: distributions are the same")
combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
points_df = combined_df[~combined_df['points'].isna()]
print(points_df['image_class'].value_counts())
print(points_df['sample_id'].value_counts().head(10))
df = initiating_defect_features.make_feature_df(points_df)
columns = ["screen_portion","max_sharpness","aspect_ratio","perimeter","pixels"]
df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)
df = df.dropna()
results_df = initiating_defect_features.regression_on_df(df)
print(results_df.loc[results_df["r2"].idxmax()])
print(results_df.loc[results_df[(results_df['aspect_ratio']==False) &(results_df['sharpness']==False)]["r2"].idxmax()])
initiating_defect_features.plot_feature_df(df[columns])
combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
points_df = combined_df[~combined_df['points'].isna()]
df = initiating_defect_features.make_feature_df(points_df)
columns = ["screen_portion","max_sharpness","aspect_ratio","perimeter","pixels"]
df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)
df = df.dropna()
# path = "/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/sam"
# # sam_checkpoint = path +"/sam_vit_h_4b8939.pth"
# url = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
# sam_checkpoint = urllib.request.urlretrieve(url)
model_type = "vit_h"
sam = sam_model_registry[model_type](checkpoint="/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/fractography_scripts/sam_vit_h_4b8939.pth")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Only move `sam` to GPU if an NVIDIA GPU is available
if torch.cuda.is_available():
sam.to(device=device)
print("Model moved to GPU.")
else:
print("No NVIDIA GPU detected. Using CPU.")
SAM = SamPredictor(sam)
np.random.seed(3)
df['xy'] = df['imgs'].apply(initiating_defect_mask_validation.find_centroid).apply(np.array)
SAM_outputs = []
cross_entropy = []
best_rows = []
'''Find Best Rows'''
df['SAM_raw_output'] =df.apply(lambda x: initiating_defect_mask_validation.process_row(x['imgs'],x['xy'],SAM),axis=1)
df['SAM_processed_output'].apply(initiating_defect_mask_validation.process_row).apply(initiating_defect_mask_validation.invert_mask)
df["cross_entropy"] = df.apply(
lambda x: torch.nn.functional.binary_cross_entropy(
torch.Tensor(cv2.resize(x['imgs'],(1024,1024))/255),
torch.Tensor(x['SAM_processed_output']/255)
)
,axis=1).apply(lambda x: x.detach().item())
for group_string, group in df.groupby(by="sample_id"):
# print(group_string+" running")
# group['SAM_output'] =group.apply(process_row,axis=1).apply(process_mask)
# group["cross_entropy"] = group.apply(
#     lambda x: torch.nn.functional.binary_cross_entropy(
#         torch.Tensor(cv2.resize(x['imgs'],(1024,1024))/255),
#         torch.Tensor(x['SAM_output']/255)
#     )
# ,axis=1)
best_rows.append(
group.loc[group['cross_entropy'].idxmin()]
)
SAM = SamPredictor(sam)
np.random.seed(3)
df['xy'] = df['imgs'].apply(initiating_defect_mask_validation.find_centroid).apply(np.array)
df['SAM_raw_output'] =df.apply(lambda x: initiating_defect_mask_validation.process_row(x['imgs'],x['xy'],SAM),axis=1)
import os
import sys
import cv2
import numpy as np
import pandas as pd
import math
import random
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import matplotlib.gridspec as gridspec
import re
import ast
import scipy
import seaborn
import math
import joblib
import torch
sys.path.append("/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/fractography_scripts")
import organize_data
import initiating_defect_features
import initiating_defect_mask_validation
from segment_anything import sam_model_registry, SamPredictor
combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
organize_data.print_column_counts(combined_df)
print(combined_df['image_class'].value_counts())
print('Unique samples:' +str(combined_df['sample_id'].nunique()))
with_polygon_df = combined_df[~combined_df['points'].isna()]
with_polygon_df['image_class'].value_counts()
with_polygon_df['sample_id'].value_counts().head(10)
xy_df = combined_df[~combined_df['energy_density_J_mm3'].isna() & ~combined_df['cycles'].isna()]
plt.rcParams.update({'font.size': 32})  # Set font size for all elements
hist_x = []
bin_name = []
for group_string, row in combined_df.groupby(['scan_power_W','scan_velocity_mm_s','test_stress_Mpa']):
row = row[['scan_power_W','scan_velocity_mm_s','test_stress_Mpa','cycles']].drop_duplicates().reset_index(drop=True)
variable_cycles = list(row['cycles'].value_counts().index)
if(len(variable_cycles)>1):
hist_x.append(variable_cycles)
bin_name.append(len(variable_cycles))
box_fig, box_ax = plt.subplots( figsize=(16, 8))
box_ax.tick_params(axis='x',bottom=False,labelbottom=False,)
# box_ax.yaxis.set_major_locator(ticker.MultipleLocator(80))
box_ax.set_ylabel('Cycles to Failure')
box_ax.set_xlabel('Unique Processing and Testing Conditions')
box_ax.boxplot(hist_x)
box_fig.show()
xy_df['cycles_stress'] = xy_df['cycles'].apply(lambda x: math.log(x))*xy_df['test_stress_Mpa']
SN_fig, SN_ax = plt.subplots(figsize=(16, 8))
SN_ax.scatter(xy_df['energy_density_J_mm3'],xy_df['cycles_stress'])
slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(xy_df['energy_density_J_mm3'],xy_df['cycles_stress'])
x = np.linspace(20, 120, 100)
SN_ax.plot(x, slope * x + intercept, label=f'Linear fit: y = {slope:.2f}x + {intercept:.2f}', color='red')
SN_ax.set_ylabel('log(Cycles)*stress [Mpa]')
SN_ax.set_xlabel('Energy Density [J/mm^3]')
SN_ax.text(SN_ax.get_xlim()[1],SN_ax.get_ylim()[1],f"R^2={r_value:.2f}", fontsize = 24,ha='right', va='top')
SN_fig.tight_layout(h_pad=3)
SN_fig.show()
cmap = plt.get_cmap('inferno')
fig, (ax_scatter,ax_hist_x) = plt.subplots(2,1,figsize=(14,10),height_ratios=[3,1])
groups = combined_df.sort_values(by='cycles').groupby(['energy_density_J_mm3'])
for i, (group, group_data) in enumerate(groups):
Cycles = list(map(math.log10,group_data['cycles']))
Stress = list(map(math.log10,group_data['test_stress_Mpa']))
ax_scatter.scatter(
Cycles,
Stress,
color=cmap(group_data['energy_density_J_mm3']/100))
ax_scatter.set_ylabel('log(Stress [Mpa])')
# ax_scatter.set_xlabel('log(Cycles)')
ax_scatter.set_title('Log-Log Scatter Plot of SN Data')
# colorbar = fig.colorbar(ax.collections[0], ax=ax)
ax_hist_x.hist(list(map(math.log10, combined_df['cycles'])), bins=30, orientation='vertical')
ax_hist_x.set_ylabel('Frequency')
ax_hist_x.set_xlabel("log(Cycles)")
plt.subplots_adjust(hspace=0)
plt.show()
x_SAM_process = []
y_SAM_process = []
x_SAM_test = []
y_SAM_test = []
x_not_SAM_process = []
y_not_SAM_process = []
x_not_SAM_test = []
y_not_SAM_test = []
for group_string, group in combined_df.groupby('sample_id'):
no_points = group[(group['image_class']=='initiation') & (group['points'].isna())]
points = group[(group['image_class']=='initiation') & (~group['points'].isna())]
if len(no_points.index)>=1:
x_not_SAM_process.append(no_points['scan_velocity_mm_s'].iloc[0])
y_not_SAM_process.append(no_points['scan_power_W'].iloc[0])
x_not_SAM_test.append(no_points['cycles'].iloc[0])
y_not_SAM_test.append(no_points['test_stress_Mpa'].iloc[0])
elif  len(points.index)>=1:
x_SAM_process.append(points['scan_velocity_mm_s'].iloc[0])
y_SAM_process.append(points['scan_power_W'].iloc[0])
x_SAM_test.append(points['cycles'].iloc[0])
y_SAM_test.append(points['test_stress_Mpa'].iloc[0])
def jitter(arr, jitter_amount=2):
return arr + np.random.uniform(-jitter_amount, jitter_amount, len(arr))
success_scatter_fig,(SN_axs,PV_axs) = plt.subplots(2,1,figsize=(14,10))
plt.figure(figsize=(14, 10))
# Process Variables Plot
SN_axs.scatter(jitter(x_not_SAM_process), jitter(y_not_SAM_process), color='red', label='No Points', alpha=0.7)
SN_axs.scatter(jitter(x_SAM_process), jitter(y_SAM_process), color='blue', label='With Points', alpha=0.7)
SN_axs.set_xlabel('Scan Velocity (mm/s)',fontsize=16)
SN_axs.set_ylabel('Scan Power (W)',fontsize=16)
SN_axs.set_title('Process Variables',fontsize=24)
SN_axs.legend(fontsize=24)
# Test Variables Plot
PV_axs.scatter(jitter(x_not_SAM_test), jitter(y_not_SAM_test), color='red', label='No Points', alpha=0.7)
PV_axs.scatter(jitter(x_SAM_test), jitter(y_SAM_test), color='blue', label='With Points', alpha=0.7)
PV_axs.set_xlabel('Cycles',fontsize=16)
PV_axs.set_ylabel('Test Stress (MPa)',fontsize=16)
PV_axs.set_title('Test Variables',fontsize=24)
PV_axs.legend(fontsize=16)
success_scatter_fig.tight_layout()
success_scatter_fig.show()
SAM_process = []
SAM_test = []
not_SAM_process = []
not_SAM_test = []
for group_string, group in combined_df.groupby('sample_id'):
no_points = group[(group['image_class']=='initiation') & (group['points'].isna())]
points = group[(group['image_class']=='initiation') & (~group['points'].isna())]
if not (len(no_points.index)>=1 and len(points.index)>=1):
if len(no_points.index)>=1:
not_SAM_process.append(no_points['energy_density_J_mm3'].iloc[0])
elif  len(points.index)>=1:
SAM_process.append(points['energy_density_J_mm3'].iloc[0])
plt.rcParams.update({'font.size': 32})  # Set font size for all elements
plt.figure(figsize=(10, 5))
# Process Variables Histograms
plt.hist(x_not_SAM_process, bins='auto', color='red', alpha=0.7, label='No Points')
plt.hist(x_SAM_process, bins='auto', color='blue', alpha=0.7, label='With Points')
plt.xlabel('Energy Density [J/mm^3]',fontsize=16)
plt.ylabel('Frequency',fontsize=16)
plt.title('Process Variables',fontsize=24)
plt.legend(fontsize=16)
plt.show()
x_not_SAM_process = np.array(x_not_SAM_process)
x_SAM_process = np.array(x_SAM_process)
x_not_SAM_process = x_not_SAM_process[~np.isnan(x_not_SAM_process)]
x_SAM_process = x_SAM_process[~np.isnan(x_SAM_process)]
statistic, p_value = scipy.stats.ks_2samp(np.array(x_not_SAM_process),np.array(x_SAM_process))
print("P value: "+str(p_value))
if p_value < 0.05:
print("Reject the null hypothesis: distributions are different")
else:
print("Fail to reject the null hypothesis: distributions are the same")
x_SAM_test = []
x_not_SAM_test = []
for group_string, group in combined_df.groupby('sample_id'):
no_points = group[(group['image_class']=='initiation') & (group['points'].isna())]
points = group[(group['image_class']=='initiation') & (~group['points'].isna())]
if not (len(no_points.index)>=1 and len(points.index)>=1):
if len(no_points.index)>=1:
cycles =no_points['cycles'].iloc[0]
stress =no_points['test_stress_Mpa'].iloc[0]
x_not_SAM_test.append(math.log(cycles)*math.log(stress))
elif  len(points.index)>=1:
cycles =points['cycles'].iloc[0]
stress =points['test_stress_Mpa'].iloc[0]
x_SAM_test.append(math.log(cycles)*math.log(stress))
plt.rcParams.update({'font.size': 32})  # Set font size for all elements
plt.figure(figsize=(10, 5))
# Process Variables Histograms
plt.subplot(1, 2, 1)
plt.hist(x_not_SAM_test, bins='auto', color='red', alpha=0.7, label='No Points')
plt.hist(x_SAM_test, bins='auto', color='blue', alpha=0.7, label='With Points')
plt.xlabel('log(cycles)*los(stress [Mpa])',fontsize=16)
plt.ylabel('Frequency',fontsize=16)
plt.title('Testing Variables',fontsize=20)
plt.legend(fontsize=16)
plt.show()
x_not_SAM_test = np.array(x_not_SAM_test)
x_SAM_test = np.array(x_SAM_test)
x_not_SAM_test = x_not_SAM_test[~np.isnan(x_not_SAM_test)]
x_SAM_test = x_SAM_test[~np.isnan(x_SAM_test)]
statistic, p_value = scipy.stats.ks_2samp(np.array(x_not_SAM_test),np.array(x_SAM_test))
print("P value: "+str(p_value))
if p_value < 0.05:
print("Reject the null hypothesis: distributions are different")
else:
print("Fail to reject the null hypothesis: distributions are the same")
combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
points_df = combined_df[~combined_df['points'].isna()]
print(points_df['image_class'].value_counts())
print(points_df['sample_id'].value_counts().head(10))
df = initiating_defect_features.make_feature_df(points_df)
columns = ["screen_portion","max_sharpness","aspect_ratio","perimeter","pixels"]
df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)
df = df.dropna()
results_df = initiating_defect_features.regression_on_df(df)
print(results_df.loc[results_df["r2"].idxmax()])
print(results_df.loc[results_df[(results_df['aspect_ratio']==False) &(results_df['sharpness']==False)]["r2"].idxmax()])
initiating_defect_features.plot_feature_df(df[columns])
combined_df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/lab-staging/mds3/AdvManu/fractography/combined_df.csv')
points_df = combined_df[~combined_df['points'].isna()]
df = initiating_defect_features.make_feature_df(points_df)
columns = ["screen_portion","max_sharpness","aspect_ratio","perimeter","pixels"]
df[columns] = df[columns].replace([np.inf, -np.inf], np.nan)
df = df.dropna()
# path = "/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/sam"
# # sam_checkpoint = path +"/sam_vit_h_4b8939.pth"
# url = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
# sam_checkpoint = urllib.request.urlretrieve(url)
model_type = "vit_h"
sam = sam_model_registry[model_type](checkpoint="/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/fractography_scripts/sam_vit_h_4b8939.pth")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Only move `sam` to GPU if an NVIDIA GPU is available
if torch.cuda.is_available():
sam.to(device=device)
print("Model moved to GPU.")
else:
print("No NVIDIA GPU detected. Using CPU.")
SAM = SamPredictor(sam)
np.random.seed(3)
df['xy'] = df['imgs'].apply(initiating_defect_mask_validation.find_centroid).apply(np.array)
best_rows = []
'''Find Best Rows'''
df['SAM_raw_output'] =df.apply(lambda x: initiating_defect_mask_validation.process_row(x['imgs'],x['xy'],SAM),axis=1)
df['SAM_processed_output'].apply(initiating_defect_mask_validation.process_row).apply(initiating_defect_mask_validation.invert_mask)
df["cross_entropy"] = df.apply(
lambda x: torch.nn.functional.binary_cross_entropy(
torch.Tensor(cv2.resize(x['imgs'],(1024,1024))/255),
torch.Tensor(x['SAM_processed_output']/255)
)
,axis=1).apply(lambda x: x.detach().item())
for group_string, group in df.groupby(by="sample_id"):
# print(group_string+" running")
# group['SAM_output'] =group.apply(process_row,axis=1).apply(process_mask)
# group["cross_entropy"] = group.apply(
#     lambda x: torch.nn.functional.binary_cross_entropy(
#         torch.Tensor(cv2.resize(x['imgs'],(1024,1024))/255),
#         torch.Tensor(x['SAM_output']/255)
#     )
# ,axis=1)
best_rows.append(
group.loc[group['cross_entropy'].idxmin()]
)
import os
import sys
import cv2
import numpy as np
import pandas as pd
import math
import random
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import matplotlib.gridspec as gridspec
import re
import ast
import scipy
import seaborn
import math
import joblib
import torch
sys.path.append("/mnt/vstor/CSE_MSE_RXF131/cradle-members/mds3/aml334/mds3-advman-2/topics/aml-fractography/fractography_scripts")
import organize_data
import initiating_defect_features
import initiating_defect_mask_validation
from segment_anything import sam_model_registry, SamPredictor
del initiating_defect_mask_validation
import initiating_defect_mask_validation
#| echo: false
#| fig-cap: "In the left figure the horizontal axis is the distance from the crack in the plane of the crack tip and the vertical axis is the stress as that position. We can see that as we get cloaser to the crack, the stress experienced at that point will go up to a maximum stress. On the right figure, this is visualized with force lines, which are meant to represent the force passing in a straight line from the bottom to the top of the material. Near the crack tip, several of the force lines are cut off, and are forced to pool near the crack tip, which causes the higher experienced stress."
library(knitr)
knitr::include_graphics("Figures/Stress_Concentration_factor.png")
#| echo: false
#| fig-cap: "a is the crack length, N is the cycle and K is the stress intensity factor, which estimates the stress experienced near the tip of the crack based on it's shape length and the applied stress. This means that d(a)/dN is the growth in crack length per cycle and ΔK is the change in stress near the crack tip at the high and low point of the stress. This linear relationship near the center is called the Paris Regime and is a property of the material."
knitr::include_graphics("Figures/Crack-growth-curve-for-three-crack-propagation-regions.png")
reticulate::repl_python()
#| echo: false
#| fig-cap: "Model Training"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/model_training.png")
#| echo: false
#| fig-cap: "Example Shown for SAM Model"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/SAM_webpage.png")
#| echo: false
#| fig-cap: "Example Use of X-AnyLabeling"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/X-AnyLabeling.png")
reticulate::repl_python()
#| echo: false
#| fig-cap: "Sharpness Figure"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/Sharpness_fig.png")
reticulate::repl_python()
#| echo: false
#| fig-cap: "In the left figure the horizontal axis is the distance from the crack in the plane of the crack tip and the vertical axis is the stress as that position. We can see that as we get cloaser to the crack, the stress experienced at that point will go up to a maximum stress. On the right figure, this is visualized with force lines, which are meant to represent the force passing in a straight line from the bottom to the top of the material. Near the crack tip, several of the force lines are cut off, and are forced to pool near the crack tip, which causes the higher experienced stress."
library(knitr)
knitr::include_graphics("Figures/Stress_Concentration_factor.png")
#| echo: false
#| fig-cap: "a is the crack length, N is the cycle and K is the stress intensity factor, which estimates the stress experienced near the tip of the crack based on it's shape length and the applied stress. This means that d(a)/dN is the growth in crack length per cycle and ΔK is the change in stress near the crack tip at the high and low point of the stress. This linear relationship near the center is called the Paris Regime and is a property of the material."
knitr::include_graphics("Figures/Crack-growth-curve-for-three-crack-propagation-regions.png")
reticulate::repl_python()
#| echo: false
#| fig-cap: "Model Training"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/model_training.png")
#| echo: false
#| fig-cap: "Example Shown for SAM Model"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/SAM_webpage.png")
#| echo: false
#| fig-cap: "Example Use of X-AnyLabeling"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/X-AnyLabeling.png")
reticulate::repl_python()
#| echo: false
#| fig-cap: "Sharpness Figure"
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
knitr::include_graphics("Figures/Sharpness_fig.png")
reticulate::repl_python()
